source("/Users/bob/C/text/functions.R")

path <- "/Users/bob/C/text/text_src/temp/movie_ratings/"

add.path <- function(s) paste(path,s,sep="")

##################################################################################
#  type counts, zipf
##################################################################################

# --- look at type frequencies, zipf plot   (zipf.pdf)
#     amazingly linear, with slope 1
#     60,255 word types before running tokenize script, 38,886 after (at min count 3)
#     With higher count threshold 10, have half as many 19288

# VOCB: Full vocabulary has 82631 types from input of 4384649 tokens on 5006 lines.                                     
# VOCB: Thresholded vocabulary of 38815 types with token count 4384649 tokens.                                                                                  
# VOCB: Position of OOV type is 9 with frequency 54342                                                                                                          
# MAIN: Vocabulary has 38815 types from 4384649 tokens, with 54342 OOV.  Most frequent are:                                                                      
#       ","->233729 "the"->213850 "."->184522 "a"->108441 "of"->97058 
#       "and"->94661 "to"->92188 "is"->72036 "in"->60992 "OOV"->54342                                      

type.cts <- sort(scan(add.path("type_freq.txt")), decreasing=TRUE)		

	x<-1:length(type.cts); y<-type.cts
	zipf.data <- data.frame(list(x=x,y=y,lx=log(x),ly=log(y)))

	plot(y~x, xlab="rank", ylab="frequency", log="xy", data=zipf.data)
	common.words <- c(",", "the",".","a","of","and", "to","is","in","OOV")
	text(0.9*x[1:5],0.7*y[1:5],common.words[1:5],cex=0.5)

	regr<-lm(ly~lx, data=zipf.data[1:500,]); coefficients(regr)
	lx <- log(x<-c(1,5000)); y <- exp(predict(regr, data.frame(lx=lx)))
	lines(x,y,col="red")


##################################################################################
#
# Response and document lengths
#
##################################################################################

# --- read data generated by 'regressor' (make dore)

	Data    <- read.table(add.path("lsa_ym.txt"), header=TRUE); dim(Data)

	n <- nrow(Data)
	logRating <- Data[,"Y"]    # file holds log prices
	rating    <- exp(Data[,"Y"])
	nTokens   <- Data[,"m"]
	logTokens <- log(nTokens)
	

# --- reviewer information
#     dropping 4 that are zero rated for Scott Renshaw  11961, 1391, 2790, 3285
	reviewer <- as.factor(c(rep("DS",1028),rep("JB",1308),rep("SRe",903-4),rep("SRh",1771)))
	col.r    <-  c(rep("red",1028),rep("green",1308),rep("blue",903-4),rep("violet",1771))

# --- lengths (m)
	mean(nTokens); fivenum(nTokens)
	hist(log10(nTokens), breaks=20)

# --- analysis of ratings, little association with length  (r = -0.05)
#     no reason for log scale on ratings, and indeed skews the ratings
	hist(rating, breaks=20)
	hist(logRating, breaks=20);		
	plot(rating ~ nTokens,col=col.r ); cor(rating,nTokens)

	plot(rating ~ logTokens,col=col.r ); cor(rating,logTokens)  # a very large outlier
	rating.s <- rating[-450]; logTokens.s <- logTokens[-450]    # cor = 0.19
	plot(rating.s ~ logTokens.s,col=col.r ); cor(rating.s,logTokens.s)  # without
	
	
# --- reviewer explains about 4.4% of ratings, 3% of log ratings (not much reason to log)
#     write more about better movies, on average
#	  some interaction, but tiny improvement to fit
	boxplot(logRating~reviewer)
	summary(regr.a <- lm(rating~reviewer ))  # 4.3%
	summary(regr.b <- lm(rating~reviewer + logTokens))  # 11%
	summary(regr.c <- lm(rating~reviewer * logTokens))  # 11+%
	
	
##################################################################################
#  
#   Raw word regression
#
##################################################################################
	
	W  <- as.matrix(read.table(add.path("w5708.txt"),header=T,as.is=T))

	colnames(W)[1:10]         	# remove the EOL column, relabel others
	W <- W[,-7]
	colnames(W)[c(1,2,5)] <- c(".period.",".comma.",".exclamation.")
	colnames(W)[1:10]

# --- simple composition analysis  
	short <- which((20<nTokens) & (nTokens<60)); length(short)
	long  <- which((80<nTokens) & (nTokens<150)); length(long)
	
	j <- 11:410
	p.long <- apply(W[long ,j],2,sum); p.long <- p.long/sum(p.long)
	p.short<- apply(W[short,j],2,sum); p.short<-p.short/sum(p.short)
	
	plot(p.long, p.short, log="xy", sub="400 common word types", pch=NA,
		xlab="Word Type Proportions, LONG docs",ylab="Word Type Proportions, SHORT docs")
	abline(a=0,b=1)	
	text(p.long, p.short,colnames(W)[j],cex=0.8)
	
	# transform as in Aitchison 82, but way too many zeros
	j <- 10:50; n.j <- length(j)
	W.common <- cbind(W[,j],nTokens-apply(W[,j],1,sum))
	W.common <- W.common/matrix(nTokens,nrow=nrow(W.common), ncol=n.j+1, byrow=FALSE)
	
	trans <- function(freq) { freq <- pmax(freq,0.0001); log(freq/freq[n.j+1]) }
	for(r in 1:nrow(W.common)) {
		W.common[r,] <- trans(W.common[r,]) }	
	j <- 4;
	d.short <- density(W.common[short,j])
	d.long  <- density(W.common[ long,j])
	plot(d.short, type="l"); lines(d.long)
	
# --- check some fits; 5th degree from C++ with centering gets diff R2
	sr.0    <- summary(r.0<-lm(logPrice ~ poly(logTokens,5)           )); sr.0
	sr      <- summary(lm(logPrice ~ poly(logTokens,5) + W[,   1:2])); sr
	sr      <- summary(lm(logPrice ~ poly(logTokens,5) + W[,  1:20])); sr
	sr.3000 <- summary(r.3000<-lm(logPrice ~ poly(logTokens,5) + W[,1:3000])); sr.3000
	predictive.r2(r.3000)
	
	anova(r.0, r.3000)
	
# --- see how well words describe length (duh)... only interesting for PCs
	sr <- summary(regr <- lm(nTokens ~ W[,1:100])); sr
	plot(regr)
	sr <- summary(lm(logPrice ~ poly(logTokens,1)           )); sr
	sr <- summary(lm(logPrice ~ poly(logTokens,5)           )); sr
	


##################################################################################
#
#     LSA
#
##################################################################################

	file    <- add.path("lsa_cca_500_p4.txt")
	LSA     <- as.matrix(read.table(file, header=TRUE)); dim(LSA)	# 5006 rows

# --- spectrum from random matrix
	sv <- as.vector(scan(add.path("lsa_cca_500_p4_sv.txt")))
	plot(sv, log="xy", xlab="Component", ylab="Singular Value")


# --- LSA analysis 		raw			log
#		wo reviewer		0.33		0.26
#       w  reviewer		0.344		0.275

	p      <- 200
	lsa    <- as.matrix(LSA[,1:p])
	
	sr.d <- summary(regr.d <- lm(rating ~ reviewer * logTokens + lsa)); sr.d
	predictive.r2(regr.d)  # 32% @ 100, 33% @ 200
	
	# quartz(width=6.5,height=3); reset()
	coef.summary.plot(sr.d, "LSA Component", omit=6) 
	

# --- sequence of R2 statistics from C++  (watch for """ in C output)
	lsa.fit<- read.table(add.path("lsa_regr_fit_no_m_for.txt"),header=T)
	plot(lsa.fit[,"AICc"])
	
	# change names (legacy C++ labels (which are words) with types)
	# rownames(lsa.fit) <- c("tokens",paste("lsa",1:(nrow(lsa.fit)-1), sep=""))  
	rownames(lsa.fit) <- c(paste("lsa",1:(nrow(lsa.fit)), sep=""))  
	
	quartz(height=3.5, width=6); reset()
	plot(word.fit[,"AICc"], type="l", xlab="Features", ylab="AICc",    # [ aic.pdf portion ]
			lty=3, ylim=range(lsa.fit[,"AICc"]))
	lines(c(opt.k,opt.k), c(0,4300), col="gray")
	lines(lsa.fit[,"AICc"]) 
	lsa.fit[opt.lsa <- which.min(lsa.fit[,"AICc"]),]; opt.lsa
	lines(c(opt.lsa,opt.lsa), c(0,3250), col="gray")
	
	p <- 523;
	lsa    <- as.matrix(LSA[,1:p])
	sr.lsa <- summary(regr.lsa <- lm(logPrice ~ poly(nTokens,5) + lsa , x=TRUE, y=TRUE)); sr.lsa
	predictive.r2(regr.lsa)


# --- residuals only hint at heteroscedasticity
plot(regr.lsa)
sres <- stdres(regr.lsa)
plot(nTokens, abs(sres))
lines(lowess(nTokens, abs(sres)), col="red")

# --- cross-validation
mse <- show.cv(regr.lsa, reps=20, seed=33213)  # use to compute first time

show.cv(regr.lsa, mse=save.mse$mse, reps=20, seed=33213)  # use if already computed

#     preliminary interactions
frame <- data.frame(logPrice,x.lsa.[,1:20])
br  <- lm(logPrice ~ .      , data = frame); summary(br)
br2 <- lm(logPrice ~ . + .*., data = frame); summary(br2)
anova(br,br2)
cor(fitted.values(regr.lsa), f <- fitted.values(br2))


