source("/Users/bob/C/text/functions.R")

path <- "/Users/bob/C/text/text_src/temp/movie_ratings/"

add.path <- function(s) paste(path,s,sep="")

##################################################################################
#  type counts, zipf
##################################################################################

# --- look at type frequencies, zipf plot   (zipf.pdf)
#     amazingly linear, with slope 1

type.cts <- sort(scan(add.path("type_freq.txt")), decreasing=TRUE)

	x<-1:length(type.cts); y<-type.cts
	zipf.data <- data.frame(list(x=x,y=y,lx=log(x),ly=log(y)))

	plot(y~x, xlab="rank", ylab="frequency", log="xy", data=zipf.data)
	common.words <- c(".", ",", "and", "-", "in")
	text(0.9*x[1:5],0.7*y[1:5],common.words,cex=c(1,1,0.5,1,0.5))

	regr<-lm(ly~lx, data=zipf.data[1:500,]); coefficients(regr)
	lx <- log(x<-c(1,5000)); y <- exp(predict(regr, data.frame(lx=lx)))
	lines(x,y,col="red")


##################################################################################
#
# Response and document lengths
#
##################################################################################

# --- read data generated by 'regressor' (make dore)

	Data    <- read.table(add.path("lsa_ym.txt"), header=TRUE); dim(Data)

	n <- nrow(Data)
	logRating <- Data[,"Y"]    # file holds log prices
	ratings   <- exp(Data[,"Y"])
	nTokens   <- Data[,"m"]
	logTokens <- log(nTokens)


# --- lengths (m)
	mean(nTokens); fivenum(nTokens); quantile(nTokens,0.87)
	boxplot(nTokens, horizontal=TRUE, xlab="Lengths of Descriptions")   # boxplot.pdf
	hist(log10(nTokens), breaks=20)


# --- analysis of ratings, not much association with length
	hist(ratings)
	plot(ratings ~ nTokens)


##################################################################################
#  
#   Raw word regression
#
##################################################################################
	
	YM <- as.matrix(read.table(add.path("lsa_ym.txt"),header=T,as.is=T))
	W  <- as.matrix(read.table(add.path("w5708.txt"),header=T,as.is=T))

	logRating <- YM[,1];
	rating    <- exp(logRating);
	nTokens   <- YM[,2];
	logTokens <- log(YM[,2])

	colnames(W)[1:10]         	# remove the EOL column, relabel others
	W <- W[,-7]
	colnames(W)[c(1,2,5)] <- c(".period.",".comma.",".exclamation.")
	colnames(W)[1:10]

# --- simple composition analysis  
	short <- which((20<nTokens) & (nTokens<60)); length(short)
	long  <- which((80<nTokens) & (nTokens<150)); length(long)
	
	j <- 11:410
	p.long <- apply(W[long ,j],2,sum); p.long <- p.long/sum(p.long)
	p.short<- apply(W[short,j],2,sum); p.short<-p.short/sum(p.short)
	
	plot(p.long, p.short, log="xy", sub="400 common word types", pch=NA,
		xlab="Word Type Proportions, LONG docs",ylab="Word Type Proportions, SHORT docs")
	abline(a=0,b=1)	
	text(p.long, p.short,colnames(W)[j],cex=0.8)
	
	# transform as in Aitchison 82, but way too many zeros
	j <- 10:50; n.j <- length(j)
	W.common <- cbind(W[,j],nTokens-apply(W[,j],1,sum))
	W.common <- W.common/matrix(nTokens,nrow=nrow(W.common), ncol=n.j+1, byrow=FALSE)
	
	trans <- function(freq) { freq <- pmax(freq,0.0001); log(freq/freq[n.j+1]) }
	for(r in 1:nrow(W.common)) {
		W.common[r,] <- trans(W.common[r,]) }	
	j <- 4;
	d.short <- density(W.common[short,j])
	d.long  <- density(W.common[ long,j])
	plot(d.short, type="l"); lines(d.long)
	
# --- check some fits; 5th degree from C++ with centering gets diff R2
	sr.0    <- summary(r.0<-lm(logPrice ~ poly(logTokens,5)           )); sr.0
	sr      <- summary(lm(logPrice ~ poly(logTokens,5) + W[,   1:2])); sr
	sr      <- summary(lm(logPrice ~ poly(logTokens,5) + W[,  1:20])); sr
	sr.3000 <- summary(r.3000<-lm(logPrice ~ poly(logTokens,5) + W[,1:3000])); sr.3000
	predictive.r2(r.3000)
	
	anova(r.0, r.3000)
	
# --- see how well words describe length (duh)... only interesting for PCs
	sr <- summary(regr <- lm(nTokens ~ W[,1:100])); sr
	plot(regr)
	sr <- summary(lm(logPrice ~ poly(logTokens,1)           )); sr
	sr <- summary(lm(logPrice ~ poly(logTokens,5)           )); sr
	


##################################################################################
#
#     LSA
#
##################################################################################

	file    <- add.path("lsa_cca_500_p4.txt")
	LSA     <- as.matrix(read.table(file, header=TRUE)); dim(LSA)


# --- LSA analysis from matrix W    adj R2=0.6567 with 1000 and log tokens

	p      <- 300
	lsa    <- as.matrix(LSA[,1:p])
	sr.lsa <- summary(regr.lsa <- lm(rating ~ lsa)); sr.lsa
	predictive.r2(regr.lsa)
	
	# quartz(width=6.5,height=3); reset()
	coef.summary.plot(sr.lsa, "LSA Component", omit=6)		# [ lsa_tstats.pdf ]  
	
# --- plot of spectrum                                      # [ spectrum.pdf ]
	sv.raw <- scan(paste(path,"svd_exact_d_raw.txt",sep=""))
	sv.row <- scan(paste(path,"svd_exact_d_row.txt",sep=""))
	sv.col <- scan(paste(path,"svd_exact_d_col.txt",sep=""))
	sv.cca <- scan(paste(path,"svd_exact_d_cca.txt",sep=""))
    
    i <- 1:2000; x <- log(i)
    plot(y <- sv.raw[i], xlab="Component", ylab="Singular Value", ylim=c(5,1000),
 				log="xy", type="l")				# solid
    y <- log(y); coefficients(lm(y ~ x))        	# -0.6     
    lines( 10 * (y<-sv.row[i]), lty=4)  		# dot dash               
    y <- log(y); coefficients(lm(y ~ x))        	# -0.6     
    lines( 10 * (y<-sv.col[i]), lty=3) 			# short dash
    y <- log(y); coefficients(lm(y ~ x))        	# -0.25    
    lines(100 * (y<-sv.cca[i]), lty=5)     		# long/short dash              
    y <- log(y); coefficients(lm(y ~ x))        	# -0.2     

# --- sequence of R2 statistics from C++  (watch for """ in C output)
	word.fit<- read.table(paste(path,"word_regr_fit_with_m_for.txt",sep=""),header=T)
	lsa.fit <- read.table(paste(path,"lsa_regr_fit_with_m_for.txt",sep=""), header=T)
	# change names (legacy C++ labels with types)
	rownames(lsa.fit) <- c("tokens",paste("lsa",1:(nrow(lsa.fit)-1), sep=""))  # allow for nTokens
	
	quartz(height=3.5, width=6); reset()
	plot(word.fit[,"AICc"], type="l", xlab="Features", ylab="AICc",    # [ aic.pdf portion ]
			lty=3, ylim=range(lsa.fit[,"AICc"]))
	lines(c(opt.k,opt.k), c(0,4300), col="gray")
	lines(lsa.fit[,"AICc"]) 
	lsa.fit[opt.lsa <- which.min(lsa.fit[,"AICc"]),]; opt.lsa
	lines(c(opt.lsa,opt.lsa), c(0,3250), col="gray")
	
	p <- 523;
	lsa    <- as.matrix(LSA[,1:p])
	sr.lsa <- summary(regr.lsa <- lm(logPrice ~ poly(nTokens,5) + lsa , x=TRUE, y=TRUE)); sr.lsa
	predictive.r2(regr.lsa)

	
# --- sequence of R2 statistics, in R
	df <- as.data.frame(cbind(logPrice,logTokens,LSA))
	
	k <- 100;
	r2.len <- rep(0,k);
	regr.len <- lm(logPrice ~ poly(logTokens,5), data=df); r2.poly <- summary(regr.len)$r.squared
	r2.none<- rep(0,k);
	regr.none<- lm(logPrice ~ 1, data=df)
	for(j in 1:k) {
		f <- paste(". ~ . + ",colnames(LSA)[j])
		regr.len  <- update(regr.len, f,data=df);
		r2.len[j] <- summary(regr.len)$r.squared;
		regr.none <- update(regr.none,f,data=df);
		r2.none[j]<- summary(regr.none)$r.squared;
		if(0 == (j%%10)) cat("j=",j,"\n")
	}
	r2.len  <- c(r2.poly,r2.len)
	r2.none <- c(  0    ,r2.none)
	
	plot(r2.len, xlim=c(0,100)); points(r2.none,col="red")
	plot((r2.len - r2.none)[1:100], ylim=c(0,0.20), 
				ylab="Increase in R2 with Length", xlab="Num LSA Terms")


correctly.ordered(logPrice, fitted.values(regr.lsa), 1000)

xtable(regr.lsa)


# --- residuals only hint at heteroscedasticity
plot(regr.lsa)
sres <- stdres(regr.lsa)
plot(nTokens, abs(sres))
lines(lowess(nTokens, abs(sres)), col="red")

# --- cross-validation
mse <- show.cv(regr.lsa, reps=20, seed=33213)  # use to compute first time

show.cv(regr.lsa, mse=save.mse$mse, reps=20, seed=33213)  # use if already computed

#     preliminary interactions
frame <- data.frame(logPrice,x.lsa.[,1:20])
br  <- lm(logPrice ~ .      , data = frame); summary(br)
br2 <- lm(logPrice ~ . + .*., data = frame); summary(br2)
anova(br,br2)
cor(fitted.values(regr.lsa), f <- fitted.values(br2))


