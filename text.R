
source("/Users/bob/C/text/functions.R")

##################################################################################
#  type counts, zipf
##################################################################################
zipf.plot <- function() { 

# --- look at type frequencies, zipf plot   (zipf.pdf)
type.cts <- sort(scan("/Users/bob/C/text/text_src/temp/type_freq.txt"), decreasing=TRUE)

x<-1:length(type.cts); y<-type.cts
zipf.data <- data.frame(list(x=x,y=y,lx=log(x),ly=log(y)))

plot(y~x, xlab="rank", ylab="frequency", log="xy", data=zipf.data)
common.words <- c(".", ",", "and", "-", "in")
text(0.9*x[1:5],0.7*y[1:5],common.words,cex=c(1,1,0.5,1,0.5))

regr<-lm(ly~lx, data=zipf.data[1:500,]); coefficients(regr)
lx <- log(x<-c(1,5000)); y <- exp(predict(regr, data.frame(lx=lx)))
lines(x,y,col="red")

}

##################################################################################
# Analysis of text regressors for real estate
##################################################################################
import.data <- function() { 

# --- read data generated by 'regressor' (make dore)
nProj <- 1500
city  <- "ChicagoOld3/"

file    <- paste("/Users/bob/C/text/text_src/temp/",city,"parsed.txt", sep="")
Data    <- read.table(file, header=TRUE); dim(Data)
file    <- paste("/Users/bob/C/text/text_src/temp/",city,"LSA_",nProj,"_raw.txt", sep="")
LSA     <- read.table(file, header=TRUE); dim(LSA)
file    <- paste("/Users/bob/C/text/text_src/temp/",city,"bigram_",nProj,".txt", sep="")
Bigram  <- read.table(file, header=TRUE); dim(Bigram)

file    <- paste("/Users/bob/C/text/text_src/temp/",city,"bigram_",nProj,"_5.txt", sep="")  # skip factor
Bigram5 <- read.table(file, header=TRUE); dim(Bigram5)
file    <- paste("/Users/bob/C/text/text_src/temp/",city,"bigram_",nProj,"_10.txt", sep="")  # skip factor
Bigram10<- read.table(file, header=TRUE); dim(Bigram5)
Eigen   <- read.table("/Users/bob/C/text/text_src/temp/eigenwords.txt", row.names=1, header=TRUE)
dim(Eigen) 

n <- nrow(Data)
logPrice  <- Data[,"Y"]    # file holds log prices
price     <- exp(Data[,"Y"])
nTokens   <- Data[,"m"]
logTokens <- log(nTokens)


# --- lengths (m)
mean(nTokens); fivenum(nTokens); quantile(nTokens,0.87)
boxplot(nTokens, horizontal=TRUE, xlab="Lengths of Descriptions")   # boxplot.pdf
hist(log10(nTokens))


# --- analysis of prices (thousands of $)
par(mfrow=c(1,2))                                             # prices.pdf
	y <- price
	hist(log10(price), breaks=30, main=" ",xlab="log10(Price)")
	qqnorm(log10(price), ylab="log10(Price)"); abline(a=mean(log10(y)),b=sd(log10(y)))
reset()

# --- simple models for log of prices has discontinuity 
plot(logPrice ~ logTokens) 
regr <- lm(logPrice ~ nTokens + logTokens); summary(regr)
lines(lowess(logTokens, logPrice, f=.3), col="red")
regr <- lm(logPrice ~ poly(logTokens,3)); summary(regr)
points(logTokens, fitted.values(regr), col="green")
}


boxplot(Bigram[,c(1,10,25,100,250,1000)]); abline(h=0,col="gray")
#     distributions appear vaguely bell-shaped
par(mfrow=c(3,1))
	hist(Bigram[,   1])
	hist(Bigram[, 100])
	hist(Bigram[,1000])
reset()
#     underlying eigenwords however have about the same variance (not centered)
ss <- function(x) { sum(x*x) }
plot(y<-apply(as.matrix( Eigen[,1:1500]  ),2,ss), ylab="(x'x)", log="xy")
x <- 1:1500
r <- lm(I(log(y)) ~ I(log(x))); r

eig<- as.matrix(Eigen[,c(1,2,3,300,800,1342)]); t(eig)%*%eig  # near Iden


#     pick coordinates at random from normal u'u=1 vectors or column of Eigen
nr <- nrow(Eigen)
vec <- rnorm(nr); vec <- vec/sqrt(sum(vec*vec))
vec <- Eigen[,10]

docs <- rep(0,10000)
for(i in 1:length(docs)) {
	ii<-sample(1:length(vec),70); # smaller as n gets larger than 70
	docs[i] <- mean(vec[ii]) }    # answer 'does not depend' on column sampled
ss(docs)                          # about 0.0016 for n=70 (as in plot of variation)
sqrt( (1/nr)/70 )                 # if mean≈0, then var is 1/nr


par(mfrow=c(2,1))
	plot(apply(as.matrix(LSA[,1:50]),2,ss), ylab="x'x")    # essentially 1 but for rounding
	plot(apply(as.matrix(LSA[,1:50]),2,sd), ylab="SD")    # vary considerably
reset()


##################################################################################
# 
# Parsed variables
#
##################################################################################
parsed.analysis <- function() {
	
sqft  <- Data[,"SqFt"];      sqft.obs <- 0<Data[,"SqFt"]         
sqft[!sqft.obs] <- mean( sqft[sqft.obs] )
baths <- Data[,"Bathrooms"]; bath.obs <- 0<Data[,"Bathrooms"]
baths[!bath.obs] <- mean( baths[bath.obs] )
beds  <- Data[,"Bedrooms"];  beds.obs <- 0<Data[,"Bedrooms"]
beds[!beds.obs] <- mean( beds[beds.obs] )

# --- percentages missing in parsed data
(n-sum(sqft.obs))/n
(n-sum(bath.obs))/n
(n-sum(beds.obs))/n

# --- plots of the parsed explanatory variables and response           parsed.pdf
show.cor <- function(x,y,X,Y,obs) {
	c <- round(cor(X,Y),2); c.obs <- round(cor(X[obs],Y[obs]),2)
	text(x,y,paste("r=",c.obs,"/",c,sep=""),cex=0.7) }
add.model <- function(x,y) {
	mod <- loess(y ~ x, span=0.75)
	p <- predict(mod); o <- order(x); 
	lines(x[o], p[o], col="red", lty=1)
	mod <- lm(y ~ poly(x,5))
	p <- predict(mod); o <- order(x); 
	lines(x[o], p[o], col="green", lty=1)
	p
	}

par(mfrow=c(2,2), mar=c(4,4,1,1), mgp=c(2,1,0))
	plot(logPrice ~ logTokens, ylab= "Log Price",  xlab="Log Length")
	  logPrice.pred <- add.model(logTokens, logPrice)
	  lines(predict(lm(logPrice,logTokens)))
	  text(1.5,6, paste("r =",round(cor(logPrice,logTokens),2)),cex=0.7)
	plot(logPrice ~ baths, ylab= "Log Price", xlab="Number Bathrooms (74% missing)", col="gray") 
	  show.cor(7,6,baths,logPrice,bath.obs)
	  points(baths[which(1==bath.obs)], logPrice[which(1==bath.obs)])  # overplot gray
	plot(logPrice ~ I(log(sqft)),  ylab= "Log Price", 
	  xlab="Log Square Feet  (94% missing)", col="gray")
	  show.cor(2,6,log(sqft),logPrice,sqft.obs)
	  points(log(sqft)[which(1==sqft.obs)], logPrice[which(1==sqft.obs)])
	plot(logPrice ~ beds , ylab= "Log Price", 
	  xlab="Number Bedrooms  (58% missing)"  , col=c("gray","black")[1+beds.obs])   
	  show.cor(7.5,6,beds,logPrice,beds.obs)
par(mfrow=c(1,1))

#     corr with sqft and logprice for not missing
cor( log(sqft)[sqft.obs], logPrice[sqft.obs] )
cor( baths[bath.obs], logPrice[bath.obs])

#     anomalies:  25 @ 265
table(nTokens)
ii <- which(nTokens==265); length(ii)  

# --- parsed regression fit

x.parsed. <- cbind(log(Data[,"m"]), log(sqft), sqft.obs, beds, beds.obs, baths, bath.obs)
colnames(x.parsed.)<-c("Log.m","Log.SqFt","SqFt.obs",
               "Bedrooms","Bedroom.obs","Bathrooms","Bathroom.obs")

summary(regr.parsed        <- lm(logPrice ~ x.parsed., x=TRUE, y=TRUE))

xtable(regr.parsed)

mse <- show.cv(regr.parsed,5)

}

##################################################################################
#  
#   Raw word regression
#
##################################################################################
word.regression <- function () {
	
	path <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/"
	YM <- as.matrix(read.table(paste(path,"lsa_ym.txt",sep=""),header=T,as.is=T))
	W  <- as.matrix(read.table(paste(path,"w5708.txt",sep=""),header=T,as.is=T))

	logPrice  <- YM[,1];
	nTokens   <- YM[,2];
	logTokens <- log(YM[,2])

# --- analysis of length effect
	plot(logTokens, logPrice)

# --- remove the EOL column
colnames(W)[1:10]
W <- W[,-7]
colnames(W)[1:10]

sr <- summary(lm(logPrice ~ nTokens + logTokens + W[,1:10]))

# --- plot cum R2 statistic, AICc  (must patch """ in source file)
r2.words.for <- read.table("/Users/bob/C/text/text_src/temp/word_regr_fit_with_m_for.txt", 
							header=TRUE, as.is=TRUE)
nrf <- nrow(r2.words.for);
r2.words.for[c(1,2,3,4,5,10,100,nrf),]
r2.words.rev <- read.table("/Users/bob/C/text/text_src/temp/word_regr_fit_with_m_rev.txt", 
							header=TRUE, as.is=TRUE)
nrr <- nrow(r2.words.rev);
r2.words.rev[c(1,2,3,4,5,10,100,nrr),]

i <- c(1,2,3,4,5,10,100,500)
cbind(r2.words.for[1+i,],r2.words.rev[nrr-i+1,])
sum(0 == diff(r2.words.for[,"RSS"]))  # how many add nothing
sum(0 == diff(r2.words.rev[,"RSS"]))  # how many add nothing

mx <- r2.words.for[nrf,"r2"]                                           # cum_r2.pdf
plot (c(0,r2.words.for[,"r2"]), type="l", xlab="Word Index", ylab="Cumulative R2")
lines(cumsum(c(r2.words.rev[1,"r2"],rev(diff(c(0,r2.words.rev[,"r2"]))[-1]))), col="gray")
lines(c(0,r2.words.rev[,"r2"]), col="black", lty=4)

plot(r2.words.for[,"AICc"], type="l", xlab="Word Index", ylab="AICc")  # aic_words.pdf
lines( r2.words.rev[,"AICc"] , col="black", lty=4)
opt.k <- which.min(r2.words.for[,"AICc"])    # 1094
lines(c(opt.k,opt.k), c(0,4500), col="gray")

# --- regression with W count matrix  (have to remove """ from type names in source)
W <- as.matrix(read.table("/Users/bob/C/text/text_src/temp/w5708.txt", header=TRUE)); dim(W)

# --- compare fits at AIC min to 2000
#  sr$r.squared  =  0.7667; 0.7708 w log   sr.1200$r.squared = 0.7020
sr     <- summary(  mwregr  <-lm(logPrice ~ nTokens + W          , x=TRUE,y=TRUE));
sr.opt <- summary(mwregr.opt<-lm(logPrice ~ nTokens + W[,1:opt.k], x=TRUE,y=TRUE));

anova(mwregr.opt,mwregr)  # F=1.93 with p<0.001 and 888,5404 df

# --- cross-validation
mse     <- show.cv(mwregr,    reps=2,seed=33213); mean(mse$mse)    #  0.7142
mse.opt <- show.cv(mwregr.opt,reps=2,seed=33213); mean(mse.opt$mse) # 0.6457

show.cv(regr.lsa, mse=save.mse$mse, reps=20, seed=33213)  # use if already computed


# --- coefs for tables 
ts <- abs(coefficients(sr)[,3])
xtable(sr$coefficients[(order(-ts)[1:15]),], digits=c(0,4,4,2,4))

#  Residual standard error: 0.6818 on 5405 degrees of freedom
#  Multiple R-squared: 0.7662,	Adjusted R-squared: 0.6807 
#  F-statistic: 8.957 on 1978 and 5405 DF,  p-value: < 2.2e-16 
	
par(mfrow=c(1,2))                     # tstatRegrInd.pdf
	y <- ts[-1]             # drop intercept
	x <- 1:length(y)        # some may be singular
	threshold <- -qnorm(.025/length(y))
	plot(x,y,	xlab="Word Column in W", ylab="|t|", main="")
	abline(h=threshold, col="gray", lty=4)
	abline(h=sqrt(2/pi), col="cyan")
	lines(lowess(x,y,f=0.3), col="red")
	half.normal.plot(y)
reset()

#     number bigger than Bonferroni; indexing is a mess due to dropped columns
vars <- sapply(names( ts[ts>threshold] )[-1], function(s) substring(s,2))
colnames(x <- W[,vars])

#     regr on just those exceeding bonferroni
summary(  bregr <- lm(logPrice ~ x )  )

# --- residuals show only a vague hint of heteroscedasticity
r <- residuals(wregr); f <- fitted.values(wregr)
plot(r,f)

plot(wregr)
}

##################################################################################
#
# LSA
#
##################################################################################
lsa.analysis <- function() {

nProj <- 1500
city  <- "ChicagoOld3/"

file    <- paste("/Users/bob/C/text/text_src/temp/",city,"lsa_raw_", nProj,".txt",sep="")
lsa    <- as.matrix(read.table(file, header=TRUE)); dim(lsa)


# --- LSA analysis from matrix W    adj R2=0.6567 with 1000 and log tokens

p    <- 1000
lsa  <- as.matrix(LSA[,1:p])
sr   <- summary(regr.lsa <- lm(logPrice ~ logTokens + lsa , x=TRUE, y=TRUE)); sr  
	
correctly.ordered(logPrice, fitted.values(regr.lsa), 1000)

xtable(regr.lsa)

par(mfrow=c(1,2))    # regrW.pdf
	y <- abs(coefficients(sr)[-(1:2),3])
	x <- 1:length(y)
	plot(x,y, 
		xlab="LSA Predictor", ylab="|t|", main="")
		abline(h=-qnorm(.025/(nProj/2)), col="gray", lty=3)
		abline(h=sqrt(2/pi), col="cyan")
		lines(lowess(x,y,f=0.3), col="red")
	half.normal.plot(y,height=5)
reset()

# --- residuals only hint at heteroscedasticity
plot(regr.lsa)
sres <- stdres(regr.lsa)
plot(nTokens, abs(sres))
lines(lowess(nTokens, abs(sres)), col="red")

# --- cross-validation
mse <- show.cv(regr.lsa, reps=20, seed=33213)  # use to compute first time

show.cv(regr.lsa, mse=save.mse$mse, reps=20, seed=33213)  # use if already computed

#     preliminary interactions
frame <- data.frame(logPrice,x.lsa.[,1:20])
br  <- lm(logPrice ~ .      , data = frame); summary(br)
br2 <- lm(logPrice ~ . + .*., data = frame); summary(br2)
anova(br,br2)
cor(fitted.values(regr.lsa), f <- fitted.values(br2))


##################################################################################
#
# Exact SVD for LSA compared to random projection
#   1500, raw counts
#
##################################################################################

exact <- c(805.262,304.603,223.003,194.768,177.421,166.183, 129.452, 123.934, 111.053,...)
			 
# read data
exact.d <- scan("~/C/text/text_src/temp/ChicagoOld3/svd_exact_d_raw.txt");
exact.u <- read.table("~/C/text/text_src/temp/ChicagoOld3/svd_exact_u_raw.txt"); dim(exact.u)  # no headers 

rp.u <- as.matrix(read.table("~/C/text/text_src/temp/ChicagoOld3/lsa_raw_1500_p1.txt",header=TRUE)); dim(rp.u)

# spectrum is diffuse
plot(exact.d[1:5000], log="xy", ylab="Singular Value")

# relate to U vectors found by random projection (not so correlated)
pairs(cbind(exact.u[,1:3], rp.u[,1:3]))
cor(cbind(exact.u[,1],rp.u[,1:5]))[1,]

# Because of diffuse spectrum, don't recover the full spaces.         approx.pdf
par(mfrow=c(3,1))
	k <- 200;
	cca <- cancor(rp.u[,1:k], exact.u[,1:k]); cca$cor[1:5]
	plot(cca$cor, xlab="CCA Component", ylab="Canonical Correlation"); 
	tau <- sum(cca$cor >= 0.90); cat(k, tau, cca$cor[tau],"\n"); abline(v=tau,col="gray",lty=3);
	k <- 400
	cca <- cancor(rp.u[,1:k], exact.u[,1:k])
	plot(cca$cor, xlab="CCA Component", ylab="Canonical Correlation"); 
	tau <- sum(cca$cor >= 0.90); cat(k, tau, cca$cor[tau],"\n"); abline(v=tau,col="gray",lty=3);
	k <- 800
	cca <- cancor(rp.u[,1:k], exact.u[,1:k])
	plot(cca$cor, xlab="CCA Component", ylab="Canonical Correlation");
	tau <- sum(cca$cor >= 0.90); cat(k, tau, cca$cor[tau],"\n"); abline(v=tau,col="gray",lty=3);
reset()

# 200 0.828271    400 0.8840447   800 0.9381313

# use R to find SVD based on the output of the random projection
k <- 400; udv <- svd(rp.u[,1:k])
cor(cbind(exact.u[,1],(udv$u)[,1:5]))[1,]

# not very close; [1]  1.00000000  0.16393849  0.11072754 -0.15511824 -0.09174562 -0.01180797


# --- but does it matter for the regression: ie, better with exact SVD?

p    <- 200
lsa  <- as.matrix(exact.u[,1:p])
sr   <- summary(regr.ex <- lm(logPrice ~ poly(logTokens,5) + lsa , x=TRUE, y=TRUE)); sr  

lsa  <- as.matrix(rp.u[,1:p])
sr   <- summary(regr.rp <- lm(logPrice ~ poly(logTokens,5) + lsa , x=TRUE, y=TRUE)); sr  

	
par(mfrow=c(1,2))    # regrW.pdf
	y <- abs(coefficients(sr)[-(1:6),3])
	x <- 1:length(y)
	plot(x,y, 
		xlab="LSA Predictor", ylab="|t|", main="")
		abline(h=-qnorm(.025/(nProj/2)), col="gray", lty=3)
		abline(h=sqrt(2/pi), col="cyan")
		lines(lowess(x,y,f=0.3), col="red")
	half.normal.plot(y,height=5)
reset()



##################################################################################
#
# SVD variables, B
#
##################################################################################
	
# --- whole model summaries   adj R2 = 0.6759 with first 1000 of left with log tokens
kb <- 1000                                            				# left and right, consider skip versions

bigr <- as.matrix(Bigram[,c(seq(1,kb/2),seq(nProj-kb/2,nProj)) ])	# first and last
bigr <- as.matrix(Bigram[,1:kb])									# first kb

sr   <- summary(regr.bigram <- lm(logPrice ~ logTokens + bigr , x=TRUE, y=TRUE)); sr  

correctly.ordered(logPrice, fitted.values(regr.bigram), 1000)


par(mfrow=c(1,2))    # regrB.pdf
	y <- abs(coefficients(sr)[-1,3])
	x <- 1:length(y)          
	plot(x,y,xlab="Predictors from Bigram", ylab="|t|", main="")
		abline(h=-qnorm(.025/(nProj/2)), col="gray", lty=3)
		abline(h=sqrt(2/pi), col="cyan")
		lines(lowess(x,y, f=0.3), col="red")
	half.normal.plot(y)
reset()

#     using just half
X <- as.matrix(Data[,paste("BL",0:(kb-1), sep="")])
summary(regr.bigram.left       <- lm(logPrice ~ X, x=TRUE,y=TRUE))  # just left

par(mfrow=c(1,2))    # regrBleft.pdf
		y <- abs(coefficients(summary(regr.bigram.left))[-1,3])
		x <- 1:length(y)
	plot(x, y, xlab="Correlation Variable from Bigram", ylab="|t|", main="")
		abline(h=-qnorm(.025/length(y)), col="gray", lty=3)
		lines(lowess(x,y), col="red")
		abline(h=sqrt(2/pi), col="cyan")
	half.normal.plot(y)
reset()

# --- CCA of bigram left/right decomp
left  <-   1        : (nProj/2)
right <- (nProj/2+1):  nProj
ccw <- cancor(x.bigram.[,left], x.bigram.[,right])
plot(ccw$cor, xlab="Index of Vector", ylab="Canonical Correlation")          # cca.pdf

cx <- x.bigram.[, left] %*% ccw$xcoef		# canonical vars
cy <- x.bigram.[,right] %*% ccw$ycoef

cor(cx[,1],cy[,1])

summary(cxregr <- lm(logPrice ~ cx )); d <- ncol(cx)

par(mfrow=c(1,2))                                              # regrBcca.pdf
	plot( x <- rep(1:d),                      
		y <- abs(coefficients(summary(cxregr))[-1,3]), 
		xlab="Canonical Variable from Bigram", ylab="|t|", main="")
		abline(h=-qnorm(.025/(nProj/2)), col="gray", lty=3)
		lines(lowess(x,y), col="red")
		abline(h=sqrt(2/pi), col="cyan")
	half.normal.plot(y, height=5)
reset()

# --- CCA of bigram left with LSA
left  <-   1        : (nProj/2)
ccw <- cancor(x.bigram.[,left], x.lsa.)
plot(ccw$cor, xlab="Index of Vector", ylab="Canonical Correlation")



##################################################################################
# 
# Combined SVD variables
#
##################################################################################

#     sweep lsa from bigram variables
bigram.left  <- x.bigram.[,1:kb]
bigram.right <- x.bigram.[,(kb+1):(2*kb)]

ccw <- cancor(bigram.left, bigram.right)
cl <- bigram.left  %*% ccw$xcoef		# canonical vars
colnames(cl) <- paste("Left",1:ncol(cl), sep="_")

summary(r1 <- lm(logPrice ~ x.lsa.))                          # adj.r2 = 0.612  ChicagoOld3
summary(r2 <- lm(logPrice ~ x.lsa. + cl ))                    #          0.68
summary(r3 <- lm(logPrice ~ x.lsa. + cl + bigram.right))      #          0.70
summary(r4 <- lm(logPrice ~ x.lsa. + cl + x.bigram.right + x.parsed.)) # 0.70


y <- abs(coefficients(summary(r3))[-1,3])
d <- length(y)
par(mfrow=c(1,2))               
	plot( 
		x <- rep(1:d), y,                     
		xlab="Variable Index", ylab="|t|", main="")
		abline(h=-qnorm(.025/d), col="gray", lty=3)
		lines(lowess(x,y,f=0.3), col="red")
		abline(h=sqrt(2/pi), col="cyan")
	half.normal.plot(y, height=5)
reset()

anova(r3,r4)

r <- residuals(r4)   #   s = 0.654
f <- fitted.values(r4)

par(mfrow=c(1,2))
	plot(f,logPrice, xlab="Fitted Values", ylab="Log Price", main="Calibration Plot"); 
	abline(a=0,b=1,col="red")
	lines(lowess(f,logPrice,f=0.2),col="cyan")

	s <- 0.654
	plot(f, abs(r), xlab="Fitted Values", ylab="Absolute Residual", main="Residual Plot"); 
	abline(h=s * sqrt(2/pi), col="red")
	lines(lowess(f,abs(r),f=0.2),col="cyan")
reset()

##################################################################################
# 2-D AIC plots
##################################################################################

cv.big <- function(n.init) {
 	path <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_53853/" 
  	cvb <- as.matrix(read.table(paste(path,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE))
  	}
cv.lsa <- function(n.init) {
 	path <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_53853/" 
  	cvb <- as.matrix(read.table(paste(path,"aic_pre_lsa_",n.init,".txt",sep=""), header=TRUE))
  	}

avg.cv.big <- function(n.init) {
	path1 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_26612/"  # identifies seed
 	path2 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_16387/" 
 	path3 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_24387/" 
 	path4 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_51379/" 
 	path5 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_31427/" 
 	path6 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_73638/" 
 	
 	path <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_73638/" 
  	cv <- read.table(paste(path1,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)

 	cv1 <- read.table(paste(path1,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	cv2 <- read.table(paste(path2,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	cv3 <- read.table(paste(path3,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	cv4 <- read.table(paste(path4,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	cv5 <- read.table(paste(path5,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	# cv6 <- read.table(paste(path6,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	if((cv1[3,3] != cv2[3,3]) |(cv1[3,3] != cv3[3,3]))  cat("ERROR: Non-cv terms do not match\n");
 	(cv1+cv2+cv3+cv4+cv5)/5
 	}
 	
avg.cv.lsa <- function(n.init) {
 	path6 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_73638/" 
  	cv6 <- read.table(paste(path6,"aic_pre_lsa_",n.init,".txt",sep=""), header=TRUE)
 	cv6
	}

 	
  D10.big <- cv.big(  10);   D10.lsa <- cv.lsa(  10)
  D20.big <- cv.big(  20);   D20.lsa <- cv.lsa(  20)
  D30.big <- cv.big(  30);   D30.lsa <- cv.lsa(  30)
  D40.big <- cv.big(  40);   D40.lsa <- cv.lsa(  40)
  D50.big <- cv.big(  50);   D50.lsa <- cv.lsa(  50)
  D75.big <- cv.big(  75);   D75.lsa <- cv.lsa(  75)
 D100.big <- cv.big( 100);  D100.lsa <- cv.lsa( 100)
 D200.big <- cv.big( 200);  D200.lsa <- cv.lsa( 200)
 D400.big <- cv.big( 400);  D400.lsa <- cv.lsa( 400)
 D600.big <- cv.big( 600);  D600.lsa <- cv.lsa( 600)
 D800.big <- cv.big( 800);  D800.lsa <- cv.lsa( 800)
 D900.big <- cv.big( 900);  D900.lsa <- cv.lsa( 900)
 D950.big <- cv.big( 950);  D950.lsa <- cv.lsa( 950)
D1000.big <- cv.big(1000); D1000.lsa <- cv.lsa(1000)
D1050.big <- cv.big(1050); D1050.lsa <- cv.lsa(1050)
D1100.big <- cv.big(1100); D1100.lsa <- cv.lsa(1100)
D1200.big <- cv.big(1200); D1200.lsa <- cv.lsa(1200)
D1300.big <- cv.big(1300); D1300.lsa <- cv.lsa(1300)
D1400.big <- cv.big(1400); D1400.lsa <- cv.lsa(1400)
D1500.big <- cv.big(1500); D1500.lsa <- cv.lsa(1500)

# --- precondition on Bigram
c <- "CVSS" ; xax <- 1:1500
colors <- rainbow(22, start=0.05, end=0.9); ci <- 1
plot(D10.big[,c], type="l", log="y", xlab="Number of LSA Variables", ylab=c,
			main="Preconditioned with Bigram variables",col=colors[ci],
			ylim=c(0.98*min(D100.big[,c]), max(D10.big[,c])))
	smth <- lowess(xax,log(D10.big[,c]),f=0.05)
	lines(smth$x,exp(smth$y), col=colors[ci]); ci <- ci+1
lines(  D20.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D30.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D40.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D50.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D75.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D100.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D200.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D400.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D600.big[,c], type="l", col=colors[ci])
 smth <- lowess(xax,log(D600.big[,c]),f=0.05)
 lines(smth$x,exp(smth$y), col=colors[ci]); ci <- ci+1
lines( D800.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D900.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste(" 900 ", min(D900.big[,c]))
lines( D950.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste(" 950 ", min(D950.big[,c]))   # 2894
lines(D1000.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1000 ", min(D1000.big[,c]))  # 2858
lines(D1050.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1050 ", min(D1050.big[,c]))  # 2831
lines(D1100.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1100 ", min(D1100.big[,c]))  # 2812
lines(D1200.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1200 ", min(D1200.big[,c]))
lines(D1300.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1300 ", min(D1300.big[,c]))
lines(D1400.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1400 ", min(D1400.big[,c]))
lines(D1500.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1500 ", min(D1500.big[,c]))

# --- precondition on LSA
c <- "CVSS" ; xax <- 1:1500
colors <- rainbow(22, start=0.05, end=0.9); ci <- 1
plot(D10.lsa[,c], type="l", log="y", xlab="Number of Bigram Variables", ylab=c,
			main="Preconditioned with LSA variables",col=colors[ci],
			ylim=c(0.98*min(D100.lsa[,c]), max(D10.lsa[,c])))
	smth <- lowess(xax,log(D10.lsa[,c]),f=0.05)
	lines(smth$x,exp(smth$y), col=colors[ci]); ci <- ci+1
lines(  D20.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D30.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D40.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D50.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D75.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D100.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D200.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D400.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D600.lsa[,c], type="l", col=colors[ci])              # best
 smth <- lowess(xax,log(D600.lsa[,c]),f=0.05)
 lines(smth$x,exp(smth$y), col=colors[ci]); ci <- ci+1
lines( D800.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D900.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste(" 900 ", min(D900.lsa[,c]))
lines( D950.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste(" 950 ", min(D950.lsa[,c]))   # 2894
lines(D1000.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1000 ", min(D1000.lsa[,c]))  # 2858
lines(D1050.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1050 ", min(D1050.lsa[,c]))  # 2831
lines(D1100.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1100 ", min(D1100.lsa[,c]))  # 2812
lines(D1200.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1200 ", min(D1200.lsa[,c]))
lines(D1300.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1300 ", min(D1300.lsa[,c]))
lines(D1400.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1400 ", min(D1400.lsa[,c]))
lines(D1500.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1500 ", min(D1500.lsa[,c]))

# --- checks: Set seed to 0 in C++ (testaic)
#        How are the bigrams computed to have decreasing variation?
#        Why is the SD of the first LSA different from others?
lsa <- as.matrix(LSA[,1:750]); big <- as.matrix(Bigram[,1:750])
summary(r<-lm(logPrice ~ nTokens))                            # matches C++
summary(r<-lm(logPrice ~ nTokens + lsa[,1:10] + big[,1:750])) # matches

D10.big[10,]   # why different?
D10.lsa[10,]


# --- correlation of AICc with CVSS
plot(D1000.big[,"AICc"],D1000.big[,"CVSS"], type="l", log="xy", xlab="AICc", ylab="CVSS")
	text(D1000.big[1,"AICc"],D1000.big[1,"CVSS"],"start", cex=0.5)
lines(D1050.big[,"AICc"], D1050.big[,"CVSS"], type="l", col="red"  )
lines(D1100.big[,"AICc"], D1000.big[,"CVSS"], type="l", col="blue"  )

plot(D100.big[,"AICc"],D100.big[,"CVSS"], type="l", log="xy", xlab="AICc", ylab="CVSS")
	text(D100[1,"AICc"],D100.big[1,"CVSS"],"start", cex=0.5)
lines(D200.big[,"AICc"],D200.big[,"CVSS"], type="l", col="blue")
lines(D400.big[,"AICc"],D400.big[,"CVSS"], type="l", col="purple" )
lines(D600.big[,"AICc"],D600.big[,"CVSS"], type="l", col="red" )

	  
# combine into data for rendering
xx  <- c(10,20,30,40,50,75,100,200,400,600,800,900,950,1000,1050,1100,1200,1300,1400)
yy  <- 1:nrow(D10)
c <- "CVSS"
mat.big <- rbind(D10.big[,c],D20.big[,c],D30.big[,c],D40.big[,c],D50.big[,c],D75.big[,c],
		D100.big[,c],D200.big[,c],D400.big[,c],D600.big[,c],D800.big[,c],D900.big[,c],D950.big[,c],
		D1000.big[,c],D1050.big[,c],D1100.big[,c],D1200.big[,c],D1300.big[,c])
mat.lsa <- rbind(D10.lsa[,c],D20.lsa[,c],D30.lsa[,c],D40.lsa[,c],D50.lsa[,c],D75.lsa[,c],
		D100.lsa[,c],D200.lsa[,c],D400.lsa[,c],D600.lsa[,c],D800.lsa[,c],D900.lsa[,c],D950.lsa[,c],
		D1000.lsa[,c],D1050.lsa[,c],D1100.lsa[,c],D1200.lsa[,c],D1300.lsa[,c])

write.table(mat.big,"~/Desktop/CVSS.big.txt")
write.table(mat.lsa,"~/Desktop/CVSS.lsa.txt")



# do some smoothing
s.f <- function(x) exp(lowess(1:1500,log(x),f=0.05)$y) # smooth on log scale
s.mat <- matrix(0,nrow=nrow(mat), ncol=ncol(mat))
for(r in 1:nrow(mat)) s.mat[r,] <- s.f(mat[r,])

plot(mat[1,], type="l", log="y", xlab="Number of LSA Variables", ylab="AICc",
			main="Soothed, Initialized with varying bigram variables",
			ylim=c(min(D1000[,"AICc"]), max(D200[,"AICc"])))
lines(s.mat[1,], col="red")
lines(s.mat[8,], col="green")
lines(s.mat[10,], col="cyan")
lines(s.mat[14,], col="blue")
lines(s.mat[15,], col="black")
write.table(s.mat,"~/Desktop/AIC_smooth.txt")


persp(xx,yy,log(mat))

contour(xx,yy,log(mat), nlevels=20, xlab="Number Bigram", ylab="Number LSA")


			 
##################################################################################
# Write SVD variables to C++
##################################################################################

write.data("/Users/bob/C/auctions/data/text/text_data.txt", 500)

write.data <- function(filename, n.cols) {
	write(n, filename)
	write.vec(logPrice, "Log Price", "role y", filename)
	write.mat(x.lsa.[,1:n.cols], "LSA"  , filename)
	write.mat( cl   [,1:n.cols], "Left" , filename)
	write.mat( cr   [,1:n.cols], "Right", filename)
	}

write.vec <- function(vector, name, desc, filename)  {
	write(name,   filename, append=TRUE)
	write(desc,   filename, append=TRUE)
	write(vector, filename, append=TRUE,ncolumns=length(vector))
	}

write.mat <- function(mat, name, filename) {
	names <- colnames(mat)
	desc  <- paste("role x stream", name)
	for (j in 1:ncol(mat)) write.vec(mat[,j], paste(name,names[j],sep="_") ,desc, filename)
	}

##################################################################################

# --- regr using CCA of two sets
summary(regr <- lm(logPrice ~ x.bigram.[,left] + x.lsa.)); 

#     these are same fits 
summary(regr  <- lm(logPrice ~ x.bigram.[,left]));
summary(regr1 <- lm(logPrice ~ cx              )); 

par(mfrow=c(1,2))                                              # regrBcca2.pdf
	y <- abs(coefficients(summary(regr))[-1,3]) 
	half.normal.plot(y, height=2)
	y1 <- abs(coefficients(summary(regr1))[-1,3])      
	half.normal.plot(y1, height=5)
reset()
plot(coefficients(regr1), coefficients(regr))

#    sweep the bigram left cca from LSA to get more orthogonal (or vice versa)
r   <- lm(x.lsa. ~ cx)
res <- residuals(r)
s   <- summary(lm(logPrice ~ cx))                      # adj.r2 = 0.63  ChicagoNew
s   <- summary(lm(logPrice ~ cx + res)); s             #          0.70
s   <- summary(lm(logPrice ~ cx + res + x.parsed.)); s #          0.705

par(mfrow=c(1,2))                                             
	d <- 2 * ncol(cx)
	plot( x <- rep(1:d),                      
		y <- abs(coefficients(s)[-1,3]), log="",
		xlab="Left CCA B, then Res LSA", ylab="|t|", main="")
		abline(h=-qnorm(.025/(nProj/2)), col="gray", lty=3)
		lines(lowess(x,y), col="red")
	half.normal.plot(y, height=5)
reset()


##################################################################################
# Comparison of regression models
##################################################################################

summary(regr.parsed        <- lm(logPrice ~ x.parsed.         ))

summary(regr.parsed.lsa    <- lm(logPrice ~ x.parsed. + x.lsa.))

summary(regr.parsed.bigram <- lm(logPrice ~ x.parsed. + x.bigram.))

summary(regr.all           <- lm(logPrice ~ x.parsed. + x.lsa. + x.bigram.))
summary(regr.all)$adj.r.squared

summary(regr.text          <- lm(logPrice ~             x.lsa. + x.bigram.))
summary(regr.text)$adj.r.squared


# --- compare nested models
anova(regr.text, regr.all)

# adding lsa/bigram to parsed + bigram/lsa
anova(regr.parsed.lsa   , regr.all)
anova(regr.parsed.bigram, regr.all)

# adding lsa/bigram to parsed
anova(regr.parsed, regr.parsed.lsa)
anova(regr.parsed, regr.parsed.bigram)

# adding parsed to bigram/lsa
anova(regr.bigram, regr.parsed.bigram)
anova(regr.lsa   , regr.parsed.lsa)


##################################################################################
# Lighthouse variables
##################################################################################

# --- estimated parsed variables in place of originals
s <- summary(r <- lm(x.parsed. ~ x.lsa. ))
for(j in 1:length(s)) {
	cat(names(s)[j]," ", s[[j]]$r.squared,"\n")  }

x.parsed.hat. <- fitted.values(r)
colnames(x.parsed.hat.) <- paste("Est",colnames(x.parsed.), sep=".")

#  pick a variable
yx <- as.data.frame(cbind(logPrice, x.parsed.))         # missing 
summary( lm(logPrice ~ Bathrooms, data=yx ) )

use <- 1 == x.parsed.[,"Bathroom.obs"]                      # few observed
yx <- as.data.frame(cbind(logPrice, x.parsed.)[use,])   # just obs
summary( lm(logPrice ~ Bathrooms, data=yx ) )

yx <- as.data.frame(cbind(logPrice, x.parsed.hat.))
plot(logPrice ~ Est.Bathrooms, data=yx )
summary( regr <- lm(logPrice ~ Est.Bathrooms, data=yx ) )
abline(regr, col="red")
text(3.5,6, paste("r=",round(sqrt(summary(regr)$r.squared),2)))

#     m is worse when estimated, but all of the others improve
j <- 2;
summary(lm(logPrice ~ x.parsed.[,j] + x.parsed.hat.[,j]))

#     all of them
summary(regr.parsed)
summary(regr.parsed.hat   <- lm(logPrice ~ x.parsed.hat.))

	
# --- canonical correlations are quite large, drop slowly
ccb <- cancor(x.lsa., x.bigram.)
plot(ccb$cor)

# --- cca of the left/right bigram variables; inverted hockey stick
#     Related to how many to keep?  
#     Clearly useful to trim left/right sets down:
#        only keep 'right' subspace that's not redundant with left
left  <-   1        : (nProj/2)
right <- (nProj/2+1):  nProj
ccw <- cancor(x.bigram.[,left], x.bigram.[,right])
plot(ccw$cor)

##################################################################################
# eigenwords
#
#   optionally generate from C++
#
##################################################################################

ewords <- read.table("/Users/bob/C/text/text_src/temp/eigenwords.txt", header=TRUE)

word.types <- ewords[,1]
eigens <- ewords[,2:30]


##################################################################################
#
# Simulate data from topic model
#
##################################################################################
                                                      topic.model <- function() { }

# --- functions

rdirichlet <- function(a) {
    y <- rgamma(length(a), a, 1)
    return(y / sum(y))
}

word.indices <- function(topics, P) { 
	# generate word by sampling distribution of topics
	M <- ncol(P); m<-length(topics) 
	z<-rep(0,m); 
	for(t in 1:m) z[t]<-sample(M,1,prob=P[topics[t],])
	z
}

# --- constants

beta <- c(1, 1, 1, 2, 2, 2, 3, 3,-2,-3) 	# topic weights
K    <- length(beta)		# number of topics

M <- 2000					# word types in vocabulary
n <- 6000					# num of documents

# --- topic distributions over words in K x M matrix P
P <- matrix(0, nrow=K, ncol=M)
                                              # generate words shared by topics
n.common <-     0                             # number shared words 
common <- c(rdirichlet( rep(2,n.common) ), rep(0,M-n.common) ) 
alpha <- rep(.05, M)			                  # dirichlet parms, small alpha are spiky
for(k in 1:K) P[k,] <- (common + rdirichlet(alpha))/2
if(n.common==0) P <- 2 * P
round(P[1:10,1:12],3); apply(P,1,sum)         # prob dist so sum to 1

#     check dependence/correlation among distributions
udv <- svd(P); plot(udv$d); 
#     common words along diagonal
round(cor(t(P)),2);  plot(jitter(P[1,]),jitter(P[2,]), main="Scatter of Two Dist")
round(udv$u[,1],4); 
plot(udv$v[,1])    # common words are the leading n.col values


# --- Z[i,] is distribution of topics in document i (random Dirichlet)
Z <- matrix(0, nrow=n, ncol=K);
for(i in 1:n) Z[i,] <- rdirichlet( rep(1/10,K) )

#     Y is the response
Y <- Z %*% beta + rnorm(n,sd=0.5)

#     check "true" model; sum of the X's = 1 so no intercept, R2 around 90%
summary(regr <- lm(Y ~ Z - 1))


# --- simulate documents; lengths of the documents (neg bin)
vocab <- paste("w",1:M,sep="")
m <- rpois(n,lambda=rgamma(n,30,1))

# words in doc assigned to topic, then pick word for that topic
docs <- rep("",n)
n.topics <- rep(0,n)
for(i in 1:n) {
	topics <- sample(K, m[i], replace=TRUE, prob=Z[i,]); 
	n.topics[i] <- length(unique(topics))
	docs[i] <- paste(round(Y[i],3), paste(vocab[word.indices(topics,P)], collapse=" "), sep=" ")
	}
# count avg number unique topics
hist(n.topics); mean(n.topics)
	
# write words to file with response
write(docs,"/Users/bob/C/text/text_src/sim/sim.txt")


# --- Analysis of C++ results
#
#      RUN C++ here ( make dosim )
#

# --- look at type frequencies, zipf plot
type.cts <- sort(scan("/Users/bob/C/text/text_src/temp/type_freq.txt"), decreasing=TRUE)

x <- log(1:length(type.cts)); y<-log(type.cts)
plot(x,y, xlab="log rank", ylab="log frequency")        # simzipfa.pdf simzipfb.pdf
abline(regr<-lm(y[1:500]~x[1:500]),col="red"); coefficients(regr)


# --- get regression data
file  <- "/Users/bob/C/text/text_src/temp/sim_regr.txt"

Data <- read.table(file, header=TRUE); dim(Data)

nTokens  <- as.numeric(Data[,"n"])		# matches m above
resp <- Data[,"Y"]						# matches Y above

nProj <- 50

x.lsa.    <- as.matrix(Data[,paste( "D",0:(nProj/2-1), sep="")])
x.bigram. <- as.matrix(cbind(Data[,paste("BR",0:(nProj/2-1), sep="")],
							Data[,paste("BL",0:(nProj/2-1), sep="")]))

summary(regr.lsa           <- lm(Y ~ x.lsa.   ))
summary(regr.bigram        <- lm(Y ~ x.bigram.))        #   better fit

plot(fitted.values(regr.lsa),fitted.values(regr.bigram))
lines(lowess(fitted.values(regr.lsa),fitted.values(regr.bigram), f=1/3),  col="red")

cor(fitted.values(regr.lsa),fitted.values(regr.bigram)) # high corr if good fits  simfits.pdf

# --- cca shows several that are large (depending on separation)
ccb <- cancor(x.lsa., x.bigram.); plot(ccb$cor)

ccx <- x.lsa. 	%*% ccb$xcoef			# canonical vars
ccy <- x.bigram.	%*% ccb$ycoef

j <- 1; cor(ccx[,j],ccy[,j]); x <- ccx[,j]

summary( lm(Y ~ ccx) )

# --- cca of the left/right bigram variables; inverted hockey stick
#     these are the same subspaces; better count of number traits
left  <-   1        : (nProj/2)
right <- (nProj/2+1):  nProj
ccw <- cancor(x.bigram.[,left], x.bigram.[,right]); 
plot(ccw$cor, xlab="Canonical Variable", ylab="Canonical Correlation")           # simccab.pdf

cx <- x.bigram.[, left] %*% ccw$xcoef		# canonical vars
cy <- x.bigram.[,right] %*% ccw$ycoef

cor(cx[,1],cy[,1])
summary( lm(Y ~ cx) )

plot( cancor(cx,ccx)$cor )

# --- canonical corr of recovery with underlying structure
cc <- cancor(X,x.bigram.); plot(cc$cor)
cc <- cancor(X,x.lsa.); plot(cc$cor)


# --- linear separation of distributions
udv <- svd(P); plot(udv$d)


##################################################################################
# Marginal distributions of types and POS
##################################################################################

data <- readLines("/Users/bob/C/text/results/margins.txt")


types <- as.factor(scan(textConnection(data[2]), what=numeric(0), sep=","))
tabulate(types)
table(types)

pos <- scan(textConnection(data[4]), what=numeric(0), sep=" ")
pos <- pos[!is.na(pos)]

hist(log(types))

hist(log(pos))

# make sure sorted in descending order
types <- sort(types, decreasing=TRUE)

# percentage in largest types
n <- sum(types)

sapply(c(50,100,250,500,750,1000), function(k) c(k,sum(types[1:k])/n))



##################################################################################
# Comparison of cluster analysis of random projection matrix
##################################################################################

proj <- read.table("/Users/bob/Desktop/kmeans_data.txt"); dim(proj)

norm <- function(x) { sqrt(sum(x*x)) }

norm(proj[  1,1:50]);  norm(proj[  1,51:100])
norm(proj[ 10,1:50]);  norm(proj[ 10,51:100])
norm(proj[100,1:50]); norm(proj[100,51:100])

km <- kmeans(proj,200,iter.max=30)



##################################################################################
# Summary t stat plots for pure noise
##################################################################################
											noise.model <- function() {}
z <- rnorm(500)

par(mfrow=c(1,2))    # noise plots
	y <- abs(z)  
	h <- -qnorm(.025/length(y) )                           
	plot( 
		x <- rep(1:length(y)), y, 
		xlab="Random Noise", ylab="|t|", main="", ylim=c(0,1.1*h))
		abline(h=h, col="gray", lty=4)
		abline(h=sqrt(2/pi), col="cyan")
		lines(lowess(x,y), col="red")
	half.normal.plot(y, height=5)
reset()



##################################################################################
# Analysis of text regressors for wine
##################################################################################
											wine.model <- function() {}
nProj <- 500

file  <- paste("/Users/bob/C/text/text_src/temp/wine_regr.txt",sep="")

Data <- read.table(file, header=TRUE); dim(Data)

n <- nrow(Data)
rating    <- Data[,"Y"]
nTokens   <- Data[,"m"]

hist(rating)  ; mean(rating)   # ≈87, more bell-shaped
hist(nTokens) ; mean(nTokens)  # ≈42

# --- regression data
kw <- 250
x.lsa.    <- as.matrix(Data[,paste("D",0:(kw-1), sep="")]); 
dim(x.lsa.)

kb <- 250
x.bigram. <- as.matrix(cbind(	Data[,paste("BL",0:(kb-1), sep="")],
									Data[,paste("BR",0:(kb-1), sep="")]  ))
dim(x.bigram.)

#    LSA regression
regr <- lm(rating ~ x.lsa.)
s <- summary(regr); s

par(mfrow=c(1,2))    # LSA
	y <- abs(coefficients(s)[-1,3])[1:(nProj/2)]                                
	plot( 
		x <- rep(1:length(y)), y, 
		xlab="Wine Regr, LSA variables", ylab="|t|", main="")
		abline(h=-qnorm(.025/length(y)), col="gray", lty=3)
		lines(lowess(x,y), col="red")
	half.normal.plot(y, height=5)
reset()

#    CCA of bigram variables
left  <-   1        : (nProj/2)
right <- (nProj/2+1):  nProj
ccw <- cancor(x.bigram.[,left], x.bigram.[,right]); 
plot(ccw$cor, xlab="Canonical Variable", ylab="Canonical Correlation")           # simccab.pdf

cl <- x.bigram.[, left] %*% ccw$xcoef		# canonical vars
cr <- x.bigram.[,right] %*% ccw$ycoef

regr.cl <- lm(rating ~ cl)
s.cl <- summary(regr.cl); s.cl

par(mfrow=c(1,2))    # Left bigram, after CCA
	y <- abs(coefficients(s.cl)[-1,3])[1:(nProj/2)]                                
	plot( 
		x <- rep(1:length(y)), y, 
		xlab="Wine Regr, Bigram variables (left, after CCA)", ylab="|t|", main="")
		abline(h=-qnorm(.025/length(y)), col="gray", lty=3)
		lines(lowess(x,y), col="red")
	half.normal.plot(y, height=5)
reset()

regr <- lm(rating ~ cl + cr)
s <- summary(regr); s









