
source("/Users/bob/C/text/functions.R")

##################################################################################
#  type counts, zipf
##################################################################################

	# --- look at type frequencies, zipf plot   (zipf.pdf)
	type.cts <- sort(scan("/Users/bob/C/text/text_src/temp/ChicagoOld3/type_freq.txt"), decreasing=TRUE)

	x<-1:length(type.cts); y<-type.cts
	zipf.data <- data.frame(list(x=x,y=y,lx=log(x),ly=log(y)))

	plot(y~x, xlab="rank", ylab="frequency", log="xy", data=zipf.data)
	common.words <- c(".", ",", "and", "-", "in")
	text(0.9*x[1:5],0.7*y[1:5],common.words,cex=c(1,1,0.5,1,0.5))

	regr<-lm(ly~lx, data=zipf.data[1:500,]); coefficients(regr)
	lx <- log(x<-c(1,5000)); y <- exp(predict(regr, data.frame(lx=lx)))
	lines(x,y,col="red")


##################################################################################
#
# Response and document lengths
#
##################################################################################
import.data <- function() { 

# --- read data generated by 'regressor' (make dore)
city  <- "ChicagoOld3/"

file    <- paste("/Users/bob/C/text/text_src/temp/",city,"parsed.txt", sep="")
Data    <- read.table(file, header=TRUE); dim(Data)

n <- nrow(Data)
logPrice  <- Data[,"Y"]    # file holds log prices
price     <- exp(Data[,"Y"])
nTokens   <- Data[,"m"]
logTokens <- log(nTokens)


# --- lengths (m)
	mean(nTokens); fivenum(nTokens); quantile(nTokens,0.87)
	boxplot(nTokens, horizontal=TRUE, xlab="Lengths of Descriptions")   # boxplot.pdf
	hist(log10(nTokens))


# --- analysis of prices (thousands of $)
	par(mfrow=c(1,2))                                             # prices.pdf
		y <- price
		hist(log10(price), breaks=30, main=" ",xlab="log10(Price)")
		qqnorm(log10(price), ylab="log10(Price)"); abline(a=mean(log10(y)),b=sd(log10(y)))
	reset()

# --- simple model for log of prices 
	plot(logPrice ~ logTokens) 
	regr <- lm(logPrice ~ poly(logTokens,5)); summary(regr)
	o <- order(nTokens)
	lines(logTokens[o], fitted.values(regr)[o], col="red")
}


boxplot(Bigram[,c(1,10,25,100,250,1000)]); abline(h=0,col="gray")
#     distributions appear vaguely bell-shaped
par(mfrow=c(3,1))
	hist(Bigram[,   1])
	hist(Bigram[, 100])
	hist(Bigram[,1000])
reset()



##################################################################################
# 
# Parsed variables
#
##################################################################################

	
	sqft  <- Data[,"SqFt"];      sqft.obs <- 0<Data[,"SqFt"]         
	sqft[!sqft.obs] <- mean( sqft[sqft.obs] )
	baths <- Data[,"Bathrooms"]; bath.obs <- 0<Data[,"Bathrooms"]
	baths[!bath.obs] <- mean( baths[bath.obs] )
	beds  <- Data[,"Bedrooms"];  beds.obs <- 0<Data[,"Bedrooms"]
	beds[!beds.obs] <- mean( beds[beds.obs] )

# --- percentages missing in parsed data
	(n-sum(sqft.obs))/n
	(n-sum(bath.obs))/n
	(n-sum(beds.obs))/n

# --- plots of the parsed explanatory variables and response                
	show.cor <- function(x,y,X,Y,obs) {
		c <- round(cor(X,Y),2); c.obs <- round(cor(X[obs],Y[obs]),2)
		text(x,y,paste("r=",c.obs,"/",c,sep=""),cex=0.7) }

	add.model <- function(x,y) {
		o <- order(x);
		# p <- predict(loess(y ~ x, span=0.75))
		# lines(x[o], p[o], col="red")
		p <- predict(lm(y ~ poly(x,5)))
		lines(x[o], p[o], col="red", lty=1)
		p
		}

	quartz(width=6, height=5)                                              # [ parsed.pdf ]   
	par(mfrow=c(2,2), mar=c(4,4,1,1), mgp=c(2,1,0))
		plot(logPrice ~ logTokens, ylab= "Log Price",  xlab="Log Length")
			logPrice.pred <- add.model(logTokens, logPrice)
	  		text(1.5,6, paste("r =",round(cor(logPrice,logTokens),2)),cex=0.7)
		plot(logPrice ~ baths, ylab= "Log Price", xlab="Number Bathrooms (74% missing)", col="gray") 
	  	  show.cor(7,6,baths,logPrice,bath.obs)
	  	  points(baths[which(1==bath.obs)], logPrice[which(1==bath.obs)])  # overplot gray
		plot(logPrice ~ I(log(sqft)),  ylab= "Log Price", 
	  		xlab="Log Square Feet  (94% missing)", col="gray")
	  		show.cor(2,6,log(sqft),logPrice,sqft.obs)
	  		points(log(sqft)[which(1==sqft.obs)], logPrice[which(1==sqft.obs)])
		plot(logPrice ~ beds , ylab= "Log Price", 
	  		xlab="Number Bedrooms  (58% missing)"  , col=c("gray","black")[1+beds.obs])   
	  		show.cor(7.5,6,beds,logPrice,beds.obs)
	par(mfrow=c(1,1))

#     corr with sqft and logprice for not missing
cor( log(sqft)[sqft.obs], logPrice[sqft.obs] )
cor( baths[bath.obs], logPrice[bath.obs])

#     anomalies:  25 @ 265
table(nTokens)
ii <- which(nTokens==265); length(ii)  

# --- parsed regression fit

	x.parsed. <- cbind(log(sqft), sqft.obs, beds, beds.obs, baths, bath.obs)
	colnames(x.parsed.)<-c("Log SqFt","SqFt obs",
               "Bedrooms","Bedroom obs","Bathrooms","Bathroom obs")

	summary(regr.parsed        <- lm(logPrice ~ poly(logTokens,5) + x.parsed., x=TRUE, y=TRUE))

	xtable(regr.parsed)

mse <- show.cv(regr.parsed,5)



##################################################################################
#  
#   Raw word regression
#
##################################################################################
	
	path <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/"
	YM <- as.matrix(read.table(paste(path,"lsa_ym.txt",sep=""),header=T,as.is=T))
	W  <- as.matrix(read.table(paste(path,"w5708.txt",sep=""),header=T,as.is=T))

	logPrice  <- YM[,1];
	nTokens   <- YM[,2];
	logTokens <- log(YM[,2])

	colnames(W)[1:10]         	# remove the EOL column, relabel others
	W <- W[,-7]
	colnames(W)[c(1,2,5)] <- c(".period.",".comma.",".exclamation.")
	colnames(W)[1:10]

# --- simple composition analysis  
	short <- which((20<nTokens) & (nTokens<60)); length(short)
	long  <- which((80<nTokens) & (nTokens<150)); length(long)
	
	j <- 11:410
	p.long <- apply(W[long ,j],2,sum); p.long <- p.long/sum(p.long)
	p.short<- apply(W[short,j],2,sum); p.short<-p.short/sum(p.short)
	
	plot(p.long, p.short, log="xy", sub="400 common word types", pch=NA,
		xlab="Word Type Proportions, LONG docs",ylab="Word Type Proportions, SHORT docs")
	abline(a=0,b=1)	
	text(p.long, p.short,colnames(W)[j],cex=0.8)
	
	# transform as in Aitchison 82, but way too many zeros
	j <- 10:50; n.j <- length(j)
	W.common <- cbind(W[,j],nTokens-apply(W[,j],1,sum))
	W.common <- W.common/matrix(nTokens,nrow=nrow(W.common), ncol=n.j+1, byrow=FALSE)
	
	trans <- function(freq) { freq <- pmax(freq,0.0001); log(freq/freq[n.j+1]) }
	for(r in 1:nrow(W.common)) {
		W.common[r,] <- trans(W.common[r,]) }	
	j <- 4;
	d.short <- density(W.common[short,j])
	d.long  <- density(W.common[ long,j])
	plot(d.short, type="l"); lines(d.long)
	
# --- check some fits; 5th degree from C++ with centering gets diff R2
	sr.0    <- summary(r.0<-lm(logPrice ~ poly(logTokens,5)           )); sr.0
	sr      <- summary(lm(logPrice ~ poly(logTokens,5) + W[,   1:2])); sr
	sr      <- summary(lm(logPrice ~ poly(logTokens,5) + W[,  1:20])); sr
	sr.3000 <- summary(r.3000<-lm(logPrice ~ poly(logTokens,5) + W[,1:3000])); sr.3000
	predictive.r2(r.3000)
	
	anova(r.0, r.3000)
	
# --- see how well words describe length (duh)... only interesting for PCs
	sr <- summary(regr <- lm(nTokens ~ W[,1:100])); sr
	plot(regr)
	sr <- summary(lm(logPrice ~ poly(logTokens,1)           )); sr
	sr <- summary(lm(logPrice ~ poly(logTokens,5)           )); sr
	
# --- sequence of R2 statistics (rest done in C++)
	W.scaled <- W / matrix(nTokens,nrow=nrow(W), ncol=ncol(W), byrow=FALSE)  # rows sum to 1
	
	df <- as.data.frame(cbind(logPrice,logTokens,W))
	
	k <- 100;
	r2.len <- rep(0,k);
	regr.len <- lm(logPrice ~ poly(logTokens,5), data=df); r2.poly<-summary(regr.len)$r.squared
	r2.none<- rep(0,k);
	regr.none<- lm(logPrice ~ 1, data=df)
	for(j in 1:k) {
		f <- paste(". ~ . + ",colnames(W)[j])
		regr.len  <- update(regr.len, f,data=df);
		r2.len[j] <- summary(regr.len)$r.squared;
		regr.none <- update(regr.none,f,data=df);
		r2.none[j]<- summary(regr.none)$r.squared;
		if(0 == (j%%10)) cat("j=",j,"\n")
	}
	r2.len  <- c(r2.poly,r2.len)
	r2.none <- c(  0    ,r2.none)
	
	plot(r2.len, xlim=c(0,100)); points(r2.none,col="red")
	plot((r2.len - r2.none)[1:100], ylim=c(0,0.20), ylab="Increase in R2 with Length", xlab="Num Words")
	
	# calibrate the model with none
	summary(regr.none)$r.squared
	yhat <- fitted.values(regr.none)
	plot(logPrice ~ yhat)
	abline(a=0,b=1,col="blue") 
	regr <- lm(logPrice ~ poly(yhat,5)); summary(regr)
	o <- order(yhat);
	lines(yhat[o],fitted.values(regr)[o],col="red")
	
# --- plot cum R2 statistic, AICc  (must patch """ in source file)
	r2.words.for <- read.table(paste(path,"word_regr_fit_with_m_for.txt",sep=""),header=TRUE, as.is=TRUE)
	nrf <- nrow(r2.words.for);
	r2.words.for[c(1,2,3,4,5,10,100,nrf),]
	r2.words.rev <- read.table(paste(path,"word_regr_fit_with_m_rev.txt",sep=""),header=TRUE, as.is=TRUE)
	nrr <- nrow(r2.words.rev);
	r2.words.rev[c(1,2,3,4,5,10,100,nrr),]

	i <- c(1,2,3,4,5,10,100,500)
	cbind(r2.words.for[1+i,],r2.words.rev[nrr-i+1,])
	sum(0 == diff(r2.words.for[,"RSS"]))  # how many add nothing
	sum(0 == diff(r2.words.rev[,"RSS"]))  # how many add nothing

	quartz(height=3.5, width=6); reset()
	mx <- r2.words.for[nrf,"r2"]						# [ cumr2.pdf ]
	plot (c(0,r2.words.for[,"r2"]), type="l", xlab="Word Frequency Rank", ylab="Cumulative R-Squared")
	lines(cumsum(c(r2.words.rev[1,"r2"],rev(diff(c(0,r2.words.rev[,"r2"]))[-1]))), col="black")
	lines(c(0,r2.words.rev[,"r2"]), col="black", lty=2)

	plot(r2.words.for[,"AICc"], type="l", xlab="Word Frequency Rank", ylab="AICc")  [ aicwords.pdf ]
	# lines( r2.words.rev[,"AICc"] , col="black", lty=4)
	opt.k <- which.min(r2.words.for[,"AICc"])    # 1094 slopes, including poly (due to singularity)
	r2.words.for[opt.k,]                         # 'outstanding' is last word
	lines(c(opt.k,opt.k), c(0,4500), col="gray")

# --- explore fit chosen by AICc
#           outstanding is last word and two intermediate are singular, leaving 1089
	opt.k <- 1092
	wopt <- W[,1:opt.k] 
	sr.opt <- summary(regr.opt<-lm(logPrice ~ poly(logTokens,5) + wopt));
	predictive.r2(regr.opt)
	
	plot(regr.opt)
	plot(fitted.values(regr.opt), abs(residuals(regr.opt)))
	
	# pick off big t's and summarize model
	quartz(width=6.5,height=3); reset();
	coef.summary.plot(sr.opt, "Word Rank", omit=6)				# [ aictstats.pdf ]
	
	# find those less than bonferroni
	pv <- coefficients(sr.opt)[,4] 
	length(j <- which(pv < .05/1089))
	coefficients(sr.opt)[j,]
	# regression on just these (tricky to get correct colmns since some dropped)
	vars <- sub("wopt","",names(j[-1]))
	m <- W[,vars]
	summary(	lm(logPrice ~ m)	)
	
# --- use forward stepwise (it will complain about singularities)
	p <- 1000
	logt <- logTokens-mean(logTokens)	
	X <- cbind(logt, logt^2, logt^3, logt^4, logt^5, W[,1:p])
	colnames(X) <- c("logToken", "logToken2", "logToken3","logToken4","logToken5",colnames(W[,1:p]))

	sw <- regsubsets(y=logPrice,x=X, nbest=1, nvmax=150, force.in=1:5, method="forward")
	ssw<- summary(sw)   # summary is more helpful
	
	# example model (which holds boolean for which are in model)
	xNames <- colnames(X)[ssw$which[2,-1]]; xNames # drop first col for intercept
	summary(lm(logPrice ~ X[,xNames]))
	
	# ferret out names of added variables
	all.names <- c("Intercept",colnames(X))
	pick.names<- rep("", length(ssw$rsq))
	pick.names[1] <- all.names[ssw$which[1,]][7]
	for(j in 2:length(pick.names)) { pick.names[j] <- all.names[which(1==(ssw$which[j,]-ssw$which[j-1,]))]}
	
	# incremental t-stats
	n <- length(logPrice)
	k <- length(ssw$rss)
	e <- residuals(lm(logPrice ~ poly(logTokens,5))); 
	rss <- c(sum(e*e),ssw$rss)    # start counting from poly model
	t.stat <- as.vector(sqrt(-diff(rss)/(rss[-1]/(df <- n-6-1:k))))
	tau <- qt(1-.025/p,  df=2000)
	plot(t.stat,col=c("black","red")[1+as.numeric(t.stat<tau)], 
		xlab="Forward Stepwise Step", ylab="t-statistic")
	text(5+1:length(t.stat), 0.2+t.stat, pick.names, cex=0.5)
	i.opt <- which(t.stat<tau)[1]
	abline(h=tau, col="red")
	
	# model picked by stepwise (78 variables added, not shown in order by stepwise, adj r2 = 0.576)
	sum(ssw$which[i.opt,-1])
	xNames <- colnames(X)[ssw$which[i.opt,-1]]; xNames # drop first col for intercept
	summary(lm(logPrice ~ X[,xNames]))
	
	
# --- effects of rank-1 update on PCA of W
#     raw counts     : centering has larger effect than I prefer, but mostly on first component
#     1/nTokens      :  much larger effect (ie, center the proportions, kinda dumb)
#     1/sqrt(nTokens):  much larger effect


	www <- W[,1:k]
	
	udv  <- svd(www)
	# compare to R's principle components
	pca  <- princomp(www)

	# center
	www.c <- www - matrix(apply(www,2,mean), nrow=nrow(www), ncol=k, byrow=TRUE)
	udvc <- svd(www.c)
	
	# check that SVD agrees with PCA if centered 
	par(mfrow=c(1,2))   
		j <- 1; 
		plot(udvc$u[,j],pca$scores[,j], main="Centered")
		j <- 2; 
		plot(udvc$u[,j],pca$scores[,j], main="Centered")
	reset()
	
	# compare centered to uncentered
	par(mfrow=c(2,2))   # effect of centering larger than I would prefer
		plot(udv$u[,1],udvc$u[,1], ylab="centered #1")
		plot(udv$u[,2],udvc$u[,2], ylab="centered #2")
		plot(udv$u[,3],udvc$u[,3], ylab="centered #3")
		plot(udv$d, udvc$d, xlab="singular values", ylab="centered singular values")
		abline(a=0,b=1)
	reset()
	
	# repeat, but after rescaled
	www <- www / matrix(sqrt(nTokens),nrow=nrow(www),ncol=ncol(www), byrow=FALSE)
	
	cancor(udv$u[,1:3],udvc$u[,1:3], xcenter=FALSE, ycenter=FALSE)
	
# --- cross-validation
mse     <- show.cv(mwregr,    reps=2,seed=33213); mean(mse$mse)     #  0.7142
mse.opt <- show.cv(mwregr.opt,reps=2,seed=33213); mean(mse.opt$mse) #  0.6457

show.cv(regr.lsa, mse=save.mse$mse, reps=20, seed=33213)  # use if already computed


# --- coefs for tables 
ts <- abs(coefficients(sr)[,3])
xtable(sr$coefficients[(order(-ts)[1:15]),], digits=c(0,4,4,2,4))

#  Residual standard error: 0.6818 on 5405 degrees of freedom
#  Multiple R-squared: 0.7662,	Adjusted R-squared: 0.6807 
#  F-statistic: 8.957 on 1978 and 5405 DF,  p-value: < 2.2e-16 
	
par(mfrow=c(1,2))                     # tstatRegrInd.pdf
	y <- ts[-1]             # drop intercept
	x <- 1:length(y)        # some may be singular
	threshold <- -qnorm(.025/length(y))
	plot(x,y,	xlab="Word Column in W", ylab="|t|", main="")
	abline(h=threshold, col="gray", lty=4)
	abline(h=sqrt(2/pi), col="cyan")
	lines(lowess(x,y,f=0.3), col="red")
	half.normal.plot(y)
reset()

#     number bigger than Bonferroni; indexing is a mess due to dropped columns
vars <- sapply(names( ts[ts>threshold] )[-1], function(s) substring(s,2))
colnames(x <- W[,vars])

#     regr on just those exceeding bonferroni
summary(  bregr <- lm(logPrice ~ x )  )

# --- residuals show only a vague hint of heteroscedasticity
r <- residuals(wregr); f <- fitted.values(wregr)
plot(r,f)

plot(wregr)


##################################################################################
#
#     LSA
#
##################################################################################

	nProj   <- 1500
	weights <- "cca"
	city    <- "ChicagoOld3/"
	path    <- paste("/Users/bob/C/text/text_src/temp/",city,sep="")
	file    <- paste(path,"lsa_",weights,"_", nProj,"_p4.txt",sep="")
	LSA     <- as.matrix(read.table(file, header=TRUE)); dim(LSA)


# --- LSA analysis from matrix W    adj R2=0.6567 with 1000 and log tokens

	p      <- 1000
	lsa    <- as.matrix(LSA[,1:p])
	sr.lsa <- summary(regr.lsa <- lm(logPrice ~ poly(nTokens,5) + lsa)); sr.lsa
	predictive.r2(regr.lsa)
	
	quartz(width=6.5,height=3); reset()
	coef.summary.plot(sr.lsa, "LSA Component", omit=6)		# [ lsa_tstats.pdf ]  
	
# --- plot of spectrum                                      # [ spectrum.pdf ]
	sv <- scan(paste(path,"svd_exact_d_cca.txt",sep=""))
    plot(sv[1:4000], xlab="Component", ylab="Singular Value", log="xy")                   

# --- sequence of R2 statistics from C++  (watch for """ in C output)
	word.fit<- read.table(paste(path,"word_regr_fit_with_m_for.txt",sep=""),header=T)
	lsa.fit <- read.table(paste(path,"lsa_regr_fit_with_m_for.txt",sep=""), header=T)
	# change names (legacy C++ labels with types)
	rownames(lsa.fit) <- c("tokens",paste("lsa",1:(nrow(lsa.fit)-1), sep=""))  # allow for nTokens
	
	quartz(height=3.5, width=6); reset()
	plot(word.fit[,"AICc"], type="l", xlab="Features", ylab="AICc",    # [ aic.pdf portion ]
			lty=3, ylim=range(lsa.fit[,"AICc"]))
	lines(c(opt.k,opt.k), c(0,4300), col="gray")
	lines(lsa.fit[,"AICc"]) 
	lsa.fit[opt.lsa <- which.min(lsa.fit[,"AICc"]),]; opt.lsa
	lines(c(opt.lsa,opt.lsa), c(0,3250), col="gray")
	
	p <- 523;
	lsa    <- as.matrix(LSA[,1:p])
	sr.lsa <- summary(regr.lsa <- lm(logPrice ~ poly(nTokens,5) + lsa , x=TRUE, y=TRUE)); sr.lsa
	predictive.r2(regr.lsa)


	
	
# --- sequence of R2 statistics, in R
	df <- as.data.frame(cbind(logPrice,logTokens,LSA))
	
	k <- 100;
	r2.len <- rep(0,k);
	regr.len <- lm(logPrice ~ poly(logTokens,5), data=df); r2.poly <- summary(regr.len)$r.squared
	r2.none<- rep(0,k);
	regr.none<- lm(logPrice ~ 1, data=df)
	for(j in 1:k) {
		f <- paste(". ~ . + ",colnames(LSA)[j])
		regr.len  <- update(regr.len, f,data=df);
		r2.len[j] <- summary(regr.len)$r.squared;
		regr.none <- update(regr.none,f,data=df);
		r2.none[j]<- summary(regr.none)$r.squared;
		if(0 == (j%%10)) cat("j=",j,"\n")
	}
	r2.len  <- c(r2.poly,r2.len)
	r2.none <- c(  0    ,r2.none)
	
	plot(r2.len, xlim=c(0,100)); points(r2.none,col="red")
	plot((r2.len - r2.none)[1:100], ylim=c(0,0.20), 
				ylab="Increase in R2 with Length", xlab="Num LSA Terms")


correctly.ordered(logPrice, fitted.values(regr.lsa), 1000)

xtable(regr.lsa)


# --- residuals only hint at heteroscedasticity
plot(regr.lsa)
sres <- stdres(regr.lsa)
plot(nTokens, abs(sres))
lines(lowess(nTokens, abs(sres)), col="red")

# --- cross-validation
mse <- show.cv(regr.lsa, reps=20, seed=33213)  # use to compute first time

show.cv(regr.lsa, mse=save.mse$mse, reps=20, seed=33213)  # use if already computed

#     preliminary interactions
frame <- data.frame(logPrice,x.lsa.[,1:20])
br  <- lm(logPrice ~ .      , data = frame); summary(br)
br2 <- lm(logPrice ~ . + .*., data = frame); summary(br2)
anova(br,br2)
cor(fitted.values(regr.lsa), f <- fitted.values(br2))


##################################################################################
#
# Exact SVD for LSA compared to random projection
#   1500, raw counts
#
##################################################################################

k<-400; plot(lsa.p2[,k],lsa.p4[,k]); cor(lsa.p2[,k],lsa.p4[,k]);  # down to 0.4 by k=400


exact <- c(805.262,304.603,223.003,194.768,177.421,166.183, 129.452, 123.934, 111.053,...)
			 
# read data
exact.d <-       scan("~/C/text/text_src/temp/ChicagoOld3/svd_exact_d_cca.txt");
exact.u <- read.table("~/C/text/text_src/temp/ChicagoOld3/svd_exact_u_cca.txt"); dim(exact.u)  # no headers 

# spectrum is diffuse                                                 [ spectrum.pdf ] 
quartz(height=3.5,width=5); reset()
plot(exact.d[1:500], log="y", ylab="Singular Value")

# relate to U vectors found by random projection (not so correlated)
pairs(cbind(exact.u[,1:3], lsa[,1:3]))
cor(cbind(exact.u[,1],rp.u[,1:5]))[1,]

# Because of diffuse spectrum, don't recover the full spaces.         [ approx.pdf ]
par(mfrow=c(3,1))
	k <- 200;
	cca <- cancor(lsa[,1:k], exact.u[,1:k]); cca$cor[1:5]
	plot(cca$cor, xlab="CCA Component", ylab="Canonical Correlation"); 
	tau <- sum(cca$cor >= 0.90); cat(k, tau, cca$cor[tau],"\n"); abline(v=tau,col="gray",lty=3);
	k <- 400
	cca <- cancor(lsa[,1:k], exact.u[,1:k])
	plot(cca$cor, xlab="CCA Component", ylab="Canonical Correlation"); 
	tau <- sum(cca$cor >= 0.90); cat(k, tau, cca$cor[tau],"\n"); abline(v=tau,col="gray",lty=3);
	k <- 800
	cca <- cancor(lsa[,1:k], exact.u[,1:k])
	plot(cca$cor, xlab="CCA Component", ylab="Canonical Correlation");
	tau <- sum(cca$cor >= 0.90); cat(k, tau, cca$cor[tau],"\n"); abline(v=tau,col="gray",lty=3);
reset()

# 200 0.828271    400 0.8840447   800 0.9381313

# use R to find SVD based on the output of the random projection
k <- 400; udv <- svd(rp.u[,1:k])
cor(cbind(exact.u[,1],(udv$u)[,1:5]))[1,]

# not very close; [1]  1.00000000  0.16393849  0.11072754 -0.15511824 -0.09174562 -0.01180797


# --- but does it matter for the regression: ie, better with exact SVD?

p    <- 250
lsa  <- as.matrix(exact.u[,1:p])
sr   <- summary(regr.ex <- lm(logPrice ~ lsa)); sr  
coef.summary.plot(sr, "Exact LSA Variables", omit=1:2)

lsa  <- as.matrix(rp.u[,1:p])
sr   <- summary(regr.rp <- lm(logPrice ~ poly(logTokens,5) + lsa , x=TRUE, y=TRUE)); sr  

	
quartz(width=6.5,height=3); reset()
par(mfrow=c(1,2))    # regrW.pdf
	y <- abs(coefficients(sr)[-(1:6),3])
	x <- 1:length(y)
	plot(x,y, 
		xlab="LSA Predictor", ylab="|t|", main="")
		abline(h=-qnorm(.025/(nProj/2)), col="gray", lty=3)
		abline(h=sqrt(2/pi), col="cyan")
		lines(lowess(x,y,f=0.3), col="red")
	half.normal.plot(y,height=5)
reset()

# --- save data for Zhuang
data <- data.frame(logPrice=logPrice,exact.u = exact.u)
write.table(data, "~/Desktop/monotone_data_orthog.txt", row.names=FALSE)

# --- revisit data sent to Zhuang

# not centered
data  <- read.table("~/C/text/text_src/temp/ChicagoOld3/monotone_data.txt", header=TRUE); 
data <- as.matrix(data); dim(data)
y <- data[,1]
x <- as.matrix(data[,3:252])
sr   <- summary(regr <- lm(y ~ x)); sr  
coef.summary.plot(sr, "Uncentered Exact SVD Variables", omit=1:2)

# centered
data.c <-read.table("~/C/text/text_src/temp/ChicagoOld3/monotone_data_orthog.txt", header=TRUE) 
data.c <- as.matrix(data.c); dim(data.c)
y.c <- data.c[,1]
x.c <- as.matrix(data.c[,2:251])
sr.c   <- summary(regr.c <- lm(y.c ~ x.c)); sr.c  
coef.summary.plot(sr.c, "Uncentered Exact SVD Variables", omit=1:2)


# almost the same
udv   <- svd(  x[,1:50]); plot(  udv$d)
udv.c <- svd(x.c[,1:50]); plot(udv.c$d)

# but basis vectors are quite different
pairs(udv$u[,1:3], udv.c$u[,1:3])
# span eventually matches
cancor(udv$u[,1:20], udv.c$u[,1:20])$cor

plot(fitted.values(regr), fitted.values(regr.c))


##################################################################################
#
#     Bigram regression
#
##################################################################################

	path <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/"

	YM <- as.matrix(read.table(paste(path,"bigram_ym.txt",sep=""),header=T,as.is=T))
	logPrice <- YM[,1]
	nTokens  <- YM[,2]

	nProj   <- 1500
	weights <- "cca"
	file    <- paste(path,"bigram_",weights,"_", nProj,"_p4.txt",sep="")
	Bigram  <- as.matrix(read.table(file, header=TRUE)); dim(Bigram)


# --- Analysis of bigram centroid regressors
	p      <- 1000
	big    <- as.matrix(Bigram[,1:p])
	sr.big <- summary(regr.big <- lm(logPrice ~ poly(nTokens,5) + big)); sr.big
	predictive.r2(regr.big)
	
	# quartz(width=6.5,height=3); reset()
	coef.summary.plot(sr.big, "Bigram Component", omit=6)		# [ bigtstats.pdf ]  
	
# --- plot of spectrum                                      # [ spectrum.pdf ]
	sv <- scan(paste(path,"svd_exact_d_col.txt",sep=""))
    plot(sv[1:4000], xlab="Component", ylab="Singular Value", log="xy")                   

# --- sequence of R2 statistics, from C++  (watch for """ label)
	word.fit<- read.table(paste(path,"word_regr_fit_with_m_for.txt",sep=""),header=T)
	lsa.fit <- read.table(paste(path,"lsa_regr_fit_with_m_for.txt",sep=""), header=T)
	big.fit <- read.table(paste(path,"bigram_regr_fit_with_m_for.txt",sep=""), header=T)
	# change names (legacy C++ labels with types)
	rownames(lsa.fit) <- c("tokens",paste("lsa",1:(nrow(lsa.fit)-1), sep=""))  # allow for nTokens
	rownames(big.fit) <- c("tokens",paste("big",1:(nrow(big.fit)-1), sep=""))  
	
	# quartz(height=3.5, width=6); reset()
	plot(word.fit[,"AICc"], type="l", xlab="Features", ylab="AICc",    # [ aic.pdf ]
			lty=3, ylim=range(lsa.fit[,"AICc"]))
	lines(lsa.fit[,"AICc"]) 
	lines(big.fit[,"AICc"], lty=2)
	word.fit[opt.k <- which.min(big.fit[,"AICc"]),]; opt.k   # offset for n tokens
	points(opt.k, word.fit[opt.k,"AICc"], col="red", pch=10)
	lsa.fit[opt.lsa <- which.min(lsa.fit[,"AICc"]),]; opt.lsa 
	points(opt.lsa, lsa.fit[opt.lsa,"AICc"], col="red", pch=10)
	big.fit[opt.big <- which.min(big.fit[,"AICc"]),]; opt.big
	points(opt.big, big.fit[opt.big,"AICc"], col="red", pch=10)
	
	p <- opt.lsa-1;  
	lsa <- as.matrix(LSA[,1:p])
	sr.lsa <- summary(regr.lsa <- lm(logPrice ~ poly(nTokens,5) + lsa))
	predictive.r2(regr.lsa) # 728, close but does not match C 0.7342424 0.7049491 0.6683420
	
	p <- opt.big-1;
	big    <- as.matrix(Bigram[,1:p])
	sr.big <- summary(regr.big <- lm(logPrice ~ poly(nTokens,5) + big))
	predictive.r2(regr.big)  # 1110, close, but not a match [1] 0.7206029 0.6709017 0.5996140



##################################################################################
#
# SVD variables, B
#
##################################################################################
	
# --- whole model summaries   adj R2 = 0.6759 with first 1000 of left with log tokens
kb <- 1000                                            				# left and right, consider skip versions

bigr <- as.matrix(Bigram[,c(seq(1,kb/2),seq(nProj-kb/2,nProj)) ])	# first and last
bigr <- as.matrix(Bigram[,1:kb])									# first kb

sr   <- summary(regr.bigram <- lm(logPrice ~ logTokens + bigr , x=TRUE, y=TRUE)); sr  

correctly.ordered(logPrice, fitted.values(regr.bigram), 1000)


par(mfrow=c(1,2))    # regrB.pdf
	y <- abs(coefficients(sr)[-1,3])
	x <- 1:length(y)          
	plot(x,y,xlab="Predictors from Bigram", ylab="|t|", main="")
		abline(h=-qnorm(.025/(nProj/2)), col="gray", lty=3)
		abline(h=sqrt(2/pi), col="cyan")
		lines(lowess(x,y, f=0.3), col="red")
	half.normal.plot(y)
reset()

#     using just half
X <- as.matrix(Data[,paste("BL",0:(kb-1), sep="")])
summary(regr.bigram.left       <- lm(logPrice ~ X, x=TRUE,y=TRUE))  # just left

par(mfrow=c(1,2))    # regrBleft.pdf
		y <- abs(coefficients(summary(regr.bigram.left))[-1,3])
		x <- 1:length(y)
	plot(x, y, xlab="Correlation Variable from Bigram", ylab="|t|", main="")
		abline(h=-qnorm(.025/length(y)), col="gray", lty=3)
		lines(lowess(x,y), col="red")
		abline(h=sqrt(2/pi), col="cyan")
	half.normal.plot(y)
reset()

# --- CCA of bigram left/right decomp
left  <-   1        : (nProj/2)
right <- (nProj/2+1):  nProj
ccw <- cancor(x.bigram.[,left], x.bigram.[,right])
plot(ccw$cor, xlab="Index of Vector", ylab="Canonical Correlation")          # cca.pdf

cx <- x.bigram.[, left] %*% ccw$xcoef		# canonical vars
cy <- x.bigram.[,right] %*% ccw$ycoef

cor(cx[,1],cy[,1])

summary(cxregr <- lm(logPrice ~ cx )); d <- ncol(cx)

par(mfrow=c(1,2))                                              # regrBcca.pdf
	plot( x <- rep(1:d),                      
		y <- abs(coefficients(summary(cxregr))[-1,3]), 
		xlab="Canonical Variable from Bigram", ylab="|t|", main="")
		abline(h=-qnorm(.025/(nProj/2)), col="gray", lty=3)
		lines(lowess(x,y), col="red")
		abline(h=sqrt(2/pi), col="cyan")
	half.normal.plot(y, height=5)
reset()

# --- CCA of bigram left with LSA
left  <-   1        : (nProj/2)
ccw <- cancor(x.bigram.[,left], x.lsa.)
plot(ccw$cor, xlab="Index of Vector", ylab="Canonical Correlation")



##################################################################################
# 
# Combined SVD variables
#
##################################################################################

#     sweep lsa from bigram variables
bigram.left  <- x.bigram.[,1:kb]
bigram.right <- x.bigram.[,(kb+1):(2*kb)]

ccw <- cancor(bigram.left, bigram.right)
cl <- bigram.left  %*% ccw$xcoef		# canonical vars
colnames(cl) <- paste("Left",1:ncol(cl), sep="_")

summary(r1 <- lm(logPrice ~ x.lsa.))                          # adj.r2 = 0.612  ChicagoOld3
summary(r2 <- lm(logPrice ~ x.lsa. + cl ))                    #          0.68
summary(r3 <- lm(logPrice ~ x.lsa. + cl + bigram.right))      #          0.70
summary(r4 <- lm(logPrice ~ x.lsa. + cl + x.bigram.right + x.parsed.)) # 0.70


y <- abs(coefficients(summary(r3))[-1,3])
d <- length(y)
par(mfrow=c(1,2))               
	plot( 
		x <- rep(1:d), y,                     
		xlab="Variable Index", ylab="|t|", main="")
		abline(h=-qnorm(.025/d), col="gray", lty=3)
		lines(lowess(x,y,f=0.3), col="red")
		abline(h=sqrt(2/pi), col="cyan")
	half.normal.plot(y, height=5)
reset()

anova(r3,r4)

r <- residuals(r4)   #   s = 0.654
f <- fitted.values(r4)

par(mfrow=c(1,2))
	plot(f,logPrice, xlab="Fitted Values", ylab="Log Price", main="Calibration Plot"); 
	abline(a=0,b=1,col="red")
	lines(lowess(f,logPrice,f=0.2),col="cyan")

	s <- 0.654
	plot(f, abs(r), xlab="Fitted Values", ylab="Absolute Residual", main="Residual Plot"); 
	abline(h=s * sqrt(2/pi), col="red")
	lines(lowess(f,abs(r),f=0.2),col="cyan")
reset()

##################################################################################
# 2-D AIC plots
##################################################################################

cv.big <- function(n.init) {
 	path <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_53853/" 
  	cvb <- as.matrix(read.table(paste(path,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE))
  	}
cv.lsa <- function(n.init) {
 	path <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_53853/" 
  	cvb <- as.matrix(read.table(paste(path,"aic_pre_lsa_",n.init,".txt",sep=""), header=TRUE))
  	}

avg.cv.big <- function(n.init) {
	path1 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_26612/"  # identifies seed
 	path2 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_16387/" 
 	path3 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_24387/" 
 	path4 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_51379/" 
 	path5 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_31427/" 
 	path6 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_73638/" 
 	
 	path <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_73638/" 
  	cv <- read.table(paste(path1,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)

 	cv1 <- read.table(paste(path1,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	cv2 <- read.table(paste(path2,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	cv3 <- read.table(paste(path3,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	cv4 <- read.table(paste(path4,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	cv5 <- read.table(paste(path5,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	# cv6 <- read.table(paste(path6,"aic_pre_big_",n.init,".txt",sep=""), header=TRUE)
 	if((cv1[3,3] != cv2[3,3]) |(cv1[3,3] != cv3[3,3]))  cat("ERROR: Non-cv terms do not match\n");
 	(cv1+cv2+cv3+cv4+cv5)/5
 	}
 	
avg.cv.lsa <- function(n.init) {
 	path6 <- "/Users/bob/C/text/text_src/temp/ChicagoOld3/cv_73638/" 
  	cv6 <- read.table(paste(path6,"aic_pre_lsa_",n.init,".txt",sep=""), header=TRUE)
 	cv6
	}

 	
  D10.big <- cv.big(  10);   D10.lsa <- cv.lsa(  10)
  D20.big <- cv.big(  20);   D20.lsa <- cv.lsa(  20)
  D30.big <- cv.big(  30);   D30.lsa <- cv.lsa(  30)
  D40.big <- cv.big(  40);   D40.lsa <- cv.lsa(  40)
  D50.big <- cv.big(  50);   D50.lsa <- cv.lsa(  50)
  D75.big <- cv.big(  75);   D75.lsa <- cv.lsa(  75)
 D100.big <- cv.big( 100);  D100.lsa <- cv.lsa( 100)
 D200.big <- cv.big( 200);  D200.lsa <- cv.lsa( 200)
 D400.big <- cv.big( 400);  D400.lsa <- cv.lsa( 400)
 D600.big <- cv.big( 600);  D600.lsa <- cv.lsa( 600)
 D800.big <- cv.big( 800);  D800.lsa <- cv.lsa( 800)
 D900.big <- cv.big( 900);  D900.lsa <- cv.lsa( 900)
 D950.big <- cv.big( 950);  D950.lsa <- cv.lsa( 950)
D1000.big <- cv.big(1000); D1000.lsa <- cv.lsa(1000)
D1050.big <- cv.big(1050); D1050.lsa <- cv.lsa(1050)
D1100.big <- cv.big(1100); D1100.lsa <- cv.lsa(1100)
D1200.big <- cv.big(1200); D1200.lsa <- cv.lsa(1200)
D1300.big <- cv.big(1300); D1300.lsa <- cv.lsa(1300)
D1400.big <- cv.big(1400); D1400.lsa <- cv.lsa(1400)
D1500.big <- cv.big(1500); D1500.lsa <- cv.lsa(1500)

# --- precondition on Bigram
c <- "CVSS" ; xax <- 1:1500
colors <- rainbow(22, start=0.05, end=0.9); ci <- 1
plot(D10.big[,c], type="l", log="y", xlab="Number of LSA Variables", ylab=c,
			main="Preconditioned with Bigram variables",col=colors[ci],
			ylim=c(0.98*min(D100.big[,c]), max(D10.big[,c])))
	smth <- lowess(xax,log(D10.big[,c]),f=0.05)
	lines(smth$x,exp(smth$y), col=colors[ci]); ci <- ci+1
lines(  D20.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D30.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D40.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D50.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D75.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D100.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D200.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D400.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D600.big[,c], type="l", col=colors[ci])
 smth <- lowess(xax,log(D600.big[,c]),f=0.05)
 lines(smth$x,exp(smth$y), col=colors[ci]); ci <- ci+1
lines( D800.big[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D900.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste(" 900 ", min(D900.big[,c]))
lines( D950.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste(" 950 ", min(D950.big[,c]))   # 2894
lines(D1000.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1000 ", min(D1000.big[,c]))  # 2858
lines(D1050.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1050 ", min(D1050.big[,c]))  # 2831
lines(D1100.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1100 ", min(D1100.big[,c]))  # 2812
lines(D1200.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1200 ", min(D1200.big[,c]))
lines(D1300.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1300 ", min(D1300.big[,c]))
lines(D1400.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1400 ", min(D1400.big[,c]))
lines(D1500.big[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1500 ", min(D1500.big[,c]))

# --- precondition on LSA
c <- "CVSS" ; xax <- 1:1500
colors <- rainbow(22, start=0.05, end=0.9); ci <- 1
plot(D10.lsa[,c], type="l", log="y", xlab="Number of Bigram Variables", ylab=c,
			main="Preconditioned with LSA variables",col=colors[ci],
			ylim=c(0.98*min(D100.lsa[,c]), max(D10.lsa[,c])))
	smth <- lowess(xax,log(D10.lsa[,c]),f=0.05)
	lines(smth$x,exp(smth$y), col=colors[ci]); ci <- ci+1
lines(  D20.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D30.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D40.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D50.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines(  D75.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D100.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D200.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D400.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D600.lsa[,c], type="l", col=colors[ci])              # best
 smth <- lowess(xax,log(D600.lsa[,c]),f=0.05)
 lines(smth$x,exp(smth$y), col=colors[ci]); ci <- ci+1
lines( D800.lsa[,c], type="l", col=colors[ci]); ci <- ci+1
lines( D900.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste(" 900 ", min(D900.lsa[,c]))
lines( D950.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste(" 950 ", min(D950.lsa[,c]))   # 2894
lines(D1000.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1000 ", min(D1000.lsa[,c]))  # 2858
lines(D1050.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1050 ", min(D1050.lsa[,c]))  # 2831
lines(D1100.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1100 ", min(D1100.lsa[,c]))  # 2812
lines(D1200.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1200 ", min(D1200.lsa[,c]))
lines(D1300.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1300 ", min(D1300.lsa[,c]))
lines(D1400.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1400 ", min(D1400.lsa[,c]))
lines(D1500.lsa[,c], type="l", col=colors[ci]); ci <- ci+1; paste("1500 ", min(D1500.lsa[,c]))

# --- checks: Set seed to 0 in C++ (testaic)
#        How are the bigrams computed to have decreasing variation?
#        Why is the SD of the first LSA different from others?
lsa <- as.matrix(LSA[,1:750]); big <- as.matrix(Bigram[,1:750])
summary(r<-lm(logPrice ~ nTokens))                            # matches C++
summary(r<-lm(logPrice ~ nTokens + lsa[,1:10] + big[,1:750])) # matches

D10.big[10,]   # why different?
D10.lsa[10,]


# --- correlation of AICc with CVSS
plot(D1000.big[,"AICc"],D1000.big[,"CVSS"], type="l", log="xy", xlab="AICc", ylab="CVSS")
	text(D1000.big[1,"AICc"],D1000.big[1,"CVSS"],"start", cex=0.5)
lines(D1050.big[,"AICc"], D1050.big[,"CVSS"], type="l", col="red"  )
lines(D1100.big[,"AICc"], D1000.big[,"CVSS"], type="l", col="blue"  )

plot(D100.big[,"AICc"],D100.big[,"CVSS"], type="l", log="xy", xlab="AICc", ylab="CVSS")
	text(D100[1,"AICc"],D100.big[1,"CVSS"],"start", cex=0.5)
lines(D200.big[,"AICc"],D200.big[,"CVSS"], type="l", col="blue")
lines(D400.big[,"AICc"],D400.big[,"CVSS"], type="l", col="purple" )
lines(D600.big[,"AICc"],D600.big[,"CVSS"], type="l", col="red" )

	  
# combine into data for rendering
xx  <- c(10,20,30,40,50,75,100,200,400,600,800,900,950,1000,1050,1100,1200,1300,1400)
yy  <- 1:nrow(D10)
c <- "CVSS"
mat.big <- rbind(D10.big[,c],D20.big[,c],D30.big[,c],D40.big[,c],D50.big[,c],D75.big[,c],
		D100.big[,c],D200.big[,c],D400.big[,c],D600.big[,c],D800.big[,c],D900.big[,c],D950.big[,c],
		D1000.big[,c],D1050.big[,c],D1100.big[,c],D1200.big[,c],D1300.big[,c])
mat.lsa <- rbind(D10.lsa[,c],D20.lsa[,c],D30.lsa[,c],D40.lsa[,c],D50.lsa[,c],D75.lsa[,c],
		D100.lsa[,c],D200.lsa[,c],D400.lsa[,c],D600.lsa[,c],D800.lsa[,c],D900.lsa[,c],D950.lsa[,c],
		D1000.lsa[,c],D1050.lsa[,c],D1100.lsa[,c],D1200.lsa[,c],D1300.lsa[,c])

write.table(mat.big,"~/Desktop/CVSS.big.txt")
write.table(mat.lsa,"~/Desktop/CVSS.lsa.txt")



# do some smoothing
s.f <- function(x) exp(lowess(1:1500,log(x),f=0.05)$y) # smooth on log scale
s.mat <- matrix(0,nrow=nrow(mat), ncol=ncol(mat))
for(r in 1:nrow(mat)) s.mat[r,] <- s.f(mat[r,])

plot(mat[1,], type="l", log="y", xlab="Number of LSA Variables", ylab="AICc",
			main="Soothed, Initialized with varying bigram variables",
			ylim=c(min(D1000[,"AICc"]), max(D200[,"AICc"])))
lines(s.mat[1,], col="red")
lines(s.mat[8,], col="green")
lines(s.mat[10,], col="cyan")
lines(s.mat[14,], col="blue")
lines(s.mat[15,], col="black")
write.table(s.mat,"~/Desktop/AIC_smooth.txt")


persp(xx,yy,log(mat))

contour(xx,yy,log(mat), nlevels=20, xlab="Number Bigram", ylab="Number LSA")


			 
##################################################################################
# Write SVD variables to C++
##################################################################################

write.data("/Users/bob/C/auctions/data/text/text_data.txt", 500)

write.data <- function(filename, n.cols) {
	write(n, filename)
	write.vec(logPrice, "Log Price", "role y", filename)
	write.mat(x.lsa.[,1:n.cols], "LSA"  , filename)
	write.mat( cl   [,1:n.cols], "Left" , filename)
	write.mat( cr   [,1:n.cols], "Right", filename)
	}

write.vec <- function(vector, name, desc, filename)  {
	write(name,   filename, append=TRUE)
	write(desc,   filename, append=TRUE)
	write(vector, filename, append=TRUE,ncolumns=length(vector))
	}

write.mat <- function(mat, name, filename) {
	names <- colnames(mat)
	desc  <- paste("role x stream", name)
	for (j in 1:ncol(mat)) write.vec(mat[,j], paste(name,names[j],sep="_") ,desc, filename)
	}

##################################################################################

# --- regr using CCA of two sets
summary(regr <- lm(logPrice ~ x.bigram.[,left] + x.lsa.)); 

#     these are same fits 
summary(regr  <- lm(logPrice ~ x.bigram.[,left]));
summary(regr1 <- lm(logPrice ~ cx              )); 

par(mfrow=c(1,2))                                              # regrBcca2.pdf
	y <- abs(coefficients(summary(regr))[-1,3]) 
	half.normal.plot(y, height=2)
	y1 <- abs(coefficients(summary(regr1))[-1,3])      
	half.normal.plot(y1, height=5)
reset()
plot(coefficients(regr1), coefficients(regr))

#    sweep the bigram left cca from LSA to get more orthogonal (or vice versa)
r   <- lm(x.lsa. ~ cx)
res <- residuals(r)
s   <- summary(lm(logPrice ~ cx))                      # adj.r2 = 0.63  ChicagoNew
s   <- summary(lm(logPrice ~ cx + res)); s             #          0.70
s   <- summary(lm(logPrice ~ cx + res + x.parsed.)); s #          0.705

par(mfrow=c(1,2))                                             
	d <- 2 * ncol(cx)
	plot( x <- rep(1:d),                      
		y <- abs(coefficients(s)[-1,3]), log="",
		xlab="Left CCA B, then Res LSA", ylab="|t|", main="")
		abline(h=-qnorm(.025/(nProj/2)), col="gray", lty=3)
		lines(lowess(x,y), col="red")
	half.normal.plot(y, height=5)
reset()


##################################################################################
# Comparison of regression models
##################################################################################

summary(regr.parsed        <- lm(logPrice ~ x.parsed.         ))

summary(regr.parsed.lsa    <- lm(logPrice ~ x.parsed. + x.lsa.))

summary(regr.parsed.bigram <- lm(logPrice ~ x.parsed. + x.bigram.))

summary(regr.all           <- lm(logPrice ~ x.parsed. + x.lsa. + x.bigram.))
summary(regr.all)$adj.r.squared

summary(regr.text          <- lm(logPrice ~             x.lsa. + x.bigram.))
summary(regr.text)$adj.r.squared


# --- compare nested models
anova(regr.text, regr.all)

# adding lsa/bigram to parsed + bigram/lsa
anova(regr.parsed.lsa   , regr.all)
anova(regr.parsed.bigram, regr.all)

# adding lsa/bigram to parsed
anova(regr.parsed, regr.parsed.lsa)
anova(regr.parsed, regr.parsed.bigram)

# adding parsed to bigram/lsa
anova(regr.bigram, regr.parsed.bigram)
anova(regr.lsa   , regr.parsed.lsa)


##################################################################################
# Lighthouse variables
##################################################################################

# --- estimated parsed variables in place of originals
s <- summary(r <- lm(x.parsed. ~ x.lsa. ))
for(j in 1:length(s)) {
	cat(names(s)[j]," ", s[[j]]$r.squared,"\n")  }

x.parsed.hat. <- fitted.values(r)
colnames(x.parsed.hat.) <- paste("Est",colnames(x.parsed.), sep=".")

#  pick a variable
yx <- as.data.frame(cbind(logPrice, x.parsed.))         # missing 
summary( lm(logPrice ~ Bathrooms, data=yx ) )

use <- 1 == x.parsed.[,"Bathroom.obs"]                      # few observed
yx <- as.data.frame(cbind(logPrice, x.parsed.)[use,])   # just obs
summary( lm(logPrice ~ Bathrooms, data=yx ) )

yx <- as.data.frame(cbind(logPrice, x.parsed.hat.))
plot(logPrice ~ Est.Bathrooms, data=yx )
summary( regr <- lm(logPrice ~ Est.Bathrooms, data=yx ) )
abline(regr, col="red")
text(3.5,6, paste("r=",round(sqrt(summary(regr)$r.squared),2)))

#     m is worse when estimated, but all of the others improve
j <- 2;
summary(lm(logPrice ~ x.parsed.[,j] + x.parsed.hat.[,j]))

#     all of them
summary(regr.parsed)
summary(regr.parsed.hat   <- lm(logPrice ~ x.parsed.hat.))

	
# --- canonical correlations are quite large, drop slowly
ccb <- cancor(x.lsa., x.bigram.)
plot(ccb$cor)

# --- cca of the left/right bigram variables; inverted hockey stick
#     Related to how many to keep?  
#     Clearly useful to trim left/right sets down:
#        only keep 'right' subspace that's not redundant with left
left  <-   1        : (nProj/2)
right <- (nProj/2+1):  nProj
ccw <- cancor(x.bigram.[,left], x.bigram.[,right])
plot(ccw$cor)

##################################################################################
# eigenwords
#
#   optionally generate from C++
#
##################################################################################

ewords <- read.table("/Users/bob/C/text/text_src/temp/eigenwords.txt", header=TRUE)

word.types <- ewords[,1]
eigens <- ewords[,2:30]


##################################################################################
# Analysis of text regressors for wine
##################################################################################
											wine.model <- function() {}
nProj <- 500

file  <- paste("/Users/bob/C/text/text_src/temp/wine_regr.txt",sep="")

Data <- read.table(file, header=TRUE); dim(Data)

n <- nrow(Data)
rating    <- Data[,"Y"]
nTokens   <- Data[,"m"]

hist(rating)  ; mean(rating)   # ≈87, more bell-shaped
hist(nTokens) ; mean(nTokens)  # ≈42

# --- regression data
kw <- 250
x.lsa.    <- as.matrix(Data[,paste("D",0:(kw-1), sep="")]); 
dim(x.lsa.)

kb <- 250
x.bigram. <- as.matrix(cbind(	Data[,paste("BL",0:(kb-1), sep="")],
									Data[,paste("BR",0:(kb-1), sep="")]  ))
dim(x.bigram.)

#    LSA regression
regr <- lm(rating ~ x.lsa.)
s <- summary(regr); s

par(mfrow=c(1,2))    # LSA
	y <- abs(coefficients(s)[-1,3])[1:(nProj/2)]                                
	plot( 
		x <- rep(1:length(y)), y, 
		xlab="Wine Regr, LSA variables", ylab="|t|", main="")
		abline(h=-qnorm(.025/length(y)), col="gray", lty=3)
		lines(lowess(x,y), col="red")
	half.normal.plot(y, height=5)
reset()

#    CCA of bigram variables
left  <-   1        : (nProj/2)
right <- (nProj/2+1):  nProj
ccw <- cancor(x.bigram.[,left], x.bigram.[,right]); 
plot(ccw$cor, xlab="Canonical Variable", ylab="Canonical Correlation")           # simccab.pdf

cl <- x.bigram.[, left] %*% ccw$xcoef		# canonical vars
cr <- x.bigram.[,right] %*% ccw$ycoef

regr.cl <- lm(rating ~ cl)
s.cl <- summary(regr.cl); s.cl

par(mfrow=c(1,2))    # Left bigram, after CCA
	y <- abs(coefficients(s.cl)[-1,3])[1:(nProj/2)]                                
	plot( 
		x <- rep(1:length(y)), y, 
		xlab="Wine Regr, Bigram variables (left, after CCA)", ylab="|t|", main="")
		abline(h=-qnorm(.025/length(y)), col="gray", lty=3)
		lines(lowess(x,y), col="red")
	half.normal.plot(y, height=5)
reset()

regr <- lm(rating ~ cl + cr)
s <- summary(regr); s








# --- Analysis of C++ results
#
#      RUN C++ here ( make dosim )
#

# --- look at type frequencies, zipf plot
type.cts <- sort(scan("/Users/bob/C/text/text_src/temp/type_freq.txt"), decreasing=TRUE)

x <- log(1:length(type.cts)); y<-log(type.cts)
plot(x,y, xlab="log rank", ylab="log frequency")        # simzipfa.pdf simzipfb.pdf
abline(regr<-lm(y[1:500]~x[1:500]),col="red"); coefficients(regr)


# --- get regression data
file  <- "/Users/bob/C/text/text_src/temp/sim_regr.txt"

Data <- read.table(file, header=TRUE); dim(Data)

nTokens  <- as.numeric(Data[,"n"])		# matches m above
resp <- Data[,"Y"]						# matches Y above

nProj <- 50

x.lsa.    <- as.matrix(Data[,paste( "D",0:(nProj/2-1), sep="")])
x.bigram. <- as.matrix(cbind(Data[,paste("BR",0:(nProj/2-1), sep="")],
							Data[,paste("BL",0:(nProj/2-1), sep="")]))

summary(regr.lsa           <- lm(Y ~ x.lsa.   ))
summary(regr.bigram        <- lm(Y ~ x.bigram.))        #   better fit

plot(fitted.values(regr.lsa),fitted.values(regr.bigram))
lines(lowess(fitted.values(regr.lsa),fitted.values(regr.bigram), f=1/3),  col="red")

cor(fitted.values(regr.lsa),fitted.values(regr.bigram)) # high corr if good fits  simfits.pdf

# --- cca shows several that are large (depending on separation)
ccb <- cancor(x.lsa., x.bigram.); plot(ccb$cor)

ccx <- x.lsa. 	%*% ccb$xcoef			# canonical vars
ccy <- x.bigram.	%*% ccb$ycoef

j <- 1; cor(ccx[,j],ccy[,j]); x <- ccx[,j]

summary( lm(Y ~ ccx) )

# --- cca of the left/right bigram variables; inverted hockey stick
#     these are the same subspaces; better count of number traits
left  <-   1        : (nProj/2)
right <- (nProj/2+1):  nProj
ccw <- cancor(x.bigram.[,left], x.bigram.[,right]); 
plot(ccw$cor, xlab="Canonical Variable", ylab="Canonical Correlation")           # simccab.pdf

cx <- x.bigram.[, left] %*% ccw$xcoef		# canonical vars
cy <- x.bigram.[,right] %*% ccw$ycoef

cor(cx[,1],cy[,1])
summary( lm(Y ~ cx) )

plot( cancor(cx,ccx)$cor )

# --- canonical corr of recovery with underlying structure
cc <- cancor(X,x.bigram.); plot(cc$cor)
cc <- cancor(X,x.lsa.); plot(cc$cor)


# --- linear separation of distributions
udv <- svd(P); plot(udv$d)


##################################################################################
# Marginal distributions of types and POS
##################################################################################

data <- readLines("/Users/bob/C/text/results/margins.txt")


types <- as.factor(scan(textConnection(data[2]), what=numeric(0), sep=","))
tabulate(types)
table(types)

pos <- scan(textConnection(data[4]), what=numeric(0), sep=" ")
pos <- pos[!is.na(pos)]

hist(log(types))

hist(log(pos))

# make sure sorted in descending order
types <- sort(types, decreasing=TRUE)

# percentage in largest types
n <- sum(types)

sapply(c(50,100,250,500,750,1000), function(k) c(k,sum(types[1:k])/n))



##################################################################################
# Comparison of cluster analysis of random projection matrix
##################################################################################

proj <- read.table("/Users/bob/Desktop/kmeans_data.txt"); dim(proj)

norm <- function(x) { sqrt(sum(x*x)) }

norm(proj[  1,1:50]);  norm(proj[  1,51:100])
norm(proj[ 10,1:50]);  norm(proj[ 10,51:100])
norm(proj[100,1:50]); norm(proj[100,51:100])

km <- kmeans(proj,200,iter.max=30)



##################################################################################
# Summary t stat plots for pure noise
##################################################################################
											noise.model <- function() {}
z <- rnorm(500)

par(mfrow=c(1,2))    # noise plots
	y <- abs(z)  
	h <- -qnorm(.025/length(y) )                           
	plot( 
		x <- rep(1:length(y)), y, 
		xlab="Random Noise", ylab="|t|", main="", ylim=c(0,1.1*h))
		abline(h=h, col="gray", lty=4)
		abline(h=sqrt(2/pi), col="cyan")
		lines(lowess(x,y), col="red")
	half.normal.plot(y, height=5)
reset()




